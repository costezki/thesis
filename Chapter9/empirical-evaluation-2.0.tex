\chapter{Empirical evaluation}
% \label{ch:evaluation}

% Chapter Introduction
% - set up the scene
% - state what is the aim of the evaluation (accuracy of the parser to segment text, 
% assign unit classes, element functions and systemic features to text segments)
% - explain how that aim is achieved (compare the segmentation, element assignment, 
% feature assignment available in the corpus with that coming from the parser)
% - how the chapter is structured  
% Main sections
% - describe the corpora
%   (provenance, annotations, size metrics, quality assessment; data examples ?)
% - describe the method of comparing parser output to corpus annotations 
% - present the evaluation data for: Mood and Transitivity elements + unit classes 
%   (focus on constituent identification)
% - present the evaluation data for: Mood and Transitivity features 
%   (focus on feature accuracy per system for the identified units)
% - *interpret evaluation data and present findings
%   (or should this be provided when each evaluation data is presented) 
% Conclusions
% - concise summary of the main findings


    This chapter aims to evaluate the Parsimonious Vole parser accuracy at generating text analysis in general; and how well it performs at unit boundary detection (i.e text segmentation), unit class assignment, element assignment and feature selections in particular. The grammar that is employed in this evaluation was introduced in Chapter \ref{ch:the-grammar} and the corpus will be introduced in Section \ref{sec:corpus}. 
    
    The evaluation data are collected by comparing the labelled segments  available in the corpus to the labelled segments in the parser output. The main measurements of parser accuracy considered here are \textit{precision} and \textit{recall} and $F_1$ scores. The parser \textit{precision} measures how many segments have been produced by the parser that are also found in manual analysis; and the parser \textit{recall} measures how many correct segments have been produced by the parser relative to the total number of produced segments. $F_1$ score is a harmonic mean of the precision and recall.
    
    The corpus used in this evaluation do not constitute a true gold standard and there are, due to different reasons, some differences to the parser. These differences will be described in detail in Section \ref{sec:differences}.
    
\noindent
\begin{minipage}{\linewidth}
\begin{lstlisting}[numbers=left,basicstyle=\small\tt, stepnumber=1,firstnumber=0,frame=single,caption=Example segment from the corpus,label=lst:exampleText1,escapeinside={(*}{*)}]
(*\textcolor{black!50}{$_{587}$}*)forced me into treatment(*\textcolor{black!50}{$_{611}$}*)
\end{lstlisting}
\end{minipage}

\noindent
\begin{minipage}{\linewidth}
\begin{lstlisting}[numbers=left,basicstyle=\small\tt, stepnumber=1,firstnumber=0,frame=single,caption=Example segment from the parser output,label=lst:exampleText2,escapeinside={(*}{*)}]
(*\textcolor{black!50}{$_{583}$}*)and forced me into treatment .(*\textcolor{black!50}{$_{612}$}*)
\end{lstlisting}
\end{minipage}    

The evaluation methodology, which will be described in detail in Section \ref{sec:evaluation-methodology}, considers perfect alignment between segment boundaries and their labels. Also it considers partial alignment of segments with the same label provided that the distance between them is not too large and there is not other better matching candidate at a shorter distance. This means that segment spans such as the ones in Listings \ref{lst:exampleText1} and \ref{lst:exampleText1} are given some credit in the alignment process. In this example, the segments are considered as partially (or closely) matching clauses. The main reason for taking segmentation discrepancies into consideration is for providing a wider evaluation ground to the systemic selections from the MOOD and TRANSITIVITY system networks. The next section describes the corpora used in current evaluation. 

\section{Evaluation corpus}
\label{sec:corpus}
    % (provenance, annotations, size metrics, quality assessment; data examples ?)
    
    This section briefly introduces the corpora used in the evaluation of the Parsimonious Vole parser. Table \ref{tab:corpus-sumary} provides a summary of the corpora descriptions.  
    
    \begin{table}[!ht]
        \centering
        \resizebox{\textwidth}{!}{%
        \begin{tabulary}{\textwidth}{cccccc}
            \toprule
            Corpus name & Meta-function & Characters & Clauses & Annotator(s) \\
            \midrule
            OCD & Mood & 16.200 & 147 &  Ela Oren \& Eugeniu Costetchi \\ 
            OE & Transitivity & 51.800 & 1503 & Anke Schultz \& Tatsiana Markovic \\ 
            OE-RRH & Transitivity & 5.600 & 157 & Anke Schultz \\
        \bottomrule
        \end{tabulary}
        }
        \caption{Evaluation corpus summary}
        \label{tab:corpus-sumary}
    \end{table}
    
    % TODO: introduce corpora
    
    
    % The OE corpus was not developed for the purpose of evaluating constituency in the current parser. Nevertheless the provided segmentation however can be used to evaluate the boundaries of constituent segments. This corpus are primitively used for evaluation of the constituent's semantic function and some TRANSITIVITY features (to the degree provided in the corpus). To enable each of these evaluations the annotation data and parser output needed to be uniformly represented in order to be in alignment with the parser output. To achieve this, feature names were harmonised with the ones from PTDB following the same adjustments as described in Section \ref{sec:claning-ptdb}. In Section \ref{sec:results},  the empirical findings of the current evaluation are described.

\subsection{OE corpus}

    The Bremen Translation Corpus (BTC) was created at the University of Bremen by Kerstin Fischer, Anatol Stefanowitsch and Anke Schulz. It consists of comparable and parallel texts. The comparable part consists of a series of newsgroup texts of about 10,000 words of English text and another 10,000 words of German, text taken from the same register. The parallel part, called EDNA, is much larger comprising about 100,000 words of parallel English-German text. Anke uses in her thesis 10,000 words of parallel text and about the same of comparable text \citep[31]{schulz2015me}. In this evaluation only the English part is considered which is called OE. It comprises 31 files spanning over 1503 clauses and 20864 words. In addition, Anke provided a set of similar annotations for the ``Little Red Riding Hood'' fairy tale, called OE-RRH, comprising 157 clauses that is included as part of OE corpus in this evaluation. 
    
    \begin{figure}[!h]
        \centering
        \includegraphics[width=.75\textwidth]{Figures/Evaluation/trans-simplified.pdf}
        \caption{The fragment of the TRANSITIVITY system network that has been used in the corpus}
        \label{fig:transitivity-simplified}
    \end{figure}

    The corpus annotations, developed by Anke Schulz and Tatsiana Markovic \citep[36]{schulz2015me}, cover Cardiff TRANSITIVITY, THEME and MODIFICATION system networks. The grammatical details and the annotation methodology are covered in detail in \citet[48-161]{schulz2015me}.
    
    For the purpose of the current evaluation only the TRANSITIVITY system was considered. The extent to which the system network is covered in the corpus annotations is limited to the top part of the original TRANSITIVITY system network. The used system network fragment is depicted in Figure \ref{fig:cardiff-transitivity}, while the whole system network was provided in the Chapter \ref{ch:the-grammar}. Employing the entire system network in the annotation process increases in difficulty as the delicacy increases due to the time needed to perform the task \citep[33]{mcenery2006corpus}. The challenge of providing delicate (or fine-grained) corpus annotations using the entire extent of a system network still has to be addressed in the SFL community. 
    

\subsection{OCD corpus}

     The first corpus (OCD) was created by Ela Oren and myself and is focused on syntactic constituency structure and clause MOOD features. The texts represent blog articles of people diagnosed with Obsessive Compulsive Disorder (OCD) who self-report on the challenge of overcoming OCD. The corpus contains four texts comprising all together 988 clauses and 8605 words. 

    \begin{figure}[!h]
        \centering
        \includegraphics[width=.85\textwidth]{Figures/Evaluation/ocd1-mood-simplified.pdf}
        \caption{The part of the MOOD system network that has been used in OCD corpus annotation}
        \label{fig:mood-ocd-simplified}
    \end{figure}
    
    % todo expand, explain
    The corpus contains selections from the system network depicted in Figure \ref{fig:mood-ocd-simplified}. It is a sub-part of the MOOD system network supported by the Parsimonious Vole parser, which was described in Chapter \ref{ch:the-grammar} and depicted in Figure \ref{fig:clause-mood}. Employing the entire system network in the annotations was difficult because as the delicacy increases the time spent for the annotation process increases drastically making the annotation process very tedious and slow.
    


\subsection{Differences between the corpus annotation and parser output}
\label{sec:differences}

    This section describes the main known differences between how the parser structures output and the methodology used to annotate the corpus. These differences are mainly due to text normalisation, treatment of conjunctions and punctuation. In OCD corpus there are also some segmentation errors described below.
    
    The Parsimonious Vole parser after receiving the raw text, first operation it performs is that of text normalisation. In this process the tabs and extra spaces between words and are reduced and special characters such as quotes, parenthesis, dashes and other orthographic characters are re-represented in a uniform way. In the OE corpus most of the text is uniformly formatted but there are few deviations. 
    

\begin{minipage}{\linewidth}
\begin{lstlisting}[numbers=left,basicstyle=\small\tt, stepnumber=1,firstnumber=0,frame=single,caption=Sample of non-normalised raw text from the corpus,label=lst:exampleText,escapeinside={(*}{*)}]
(*\textcolor{black!50}{$_{0}$}*)Red riding hood excerpt(*\textcolor{black!50}{$_{24}$}*)
(*\textcolor{black!50}{$_{25}$}*)"What have you in that basket,   Little Red Riding Hood?"(*\textcolor{black!50}{$_{82}$}*)
(*\textcolor{black!50}{$_{83}$}*)
(*\textcolor{black!50}{$_{84}$}*)"Eggs and butter and cake, Mr. Wolf."(*\textcolor{black!50}{$_{111}$}*)
\end{lstlisting}
\end{minipage}

    Listing \ref{lst:exampleText} presents an example raw text from the annotation dataset containing an initial title line and two sentences separated by an empty line. The greyed index numbers at the beginning and end of each line indicate character offsets. In OE corpus files, the first line plays the role of a header containing the title or the file name. In this example it is a title. Either way, this first line is neither considered for annotation nor parsing. 
    
    In the OCD corpus normalisation was not at all addressed before the annotation started. Mostly it is organised as one sentence per line, but there are instances of extra blank lines or missing new line characters leading to few sentences per line as a block. The text may also contain tabs and extra blank spaces or blank lines as in Listing \ref{lst:exampleText} at index [82,84]. 

    It is noteworthy to mention that there are segmentation errors in a few cases from the OCD corpus. Some segments are either shifted and include the adjacent spaces (e.g. `` getting this push'' instead of ``getting this push'') or, the converse, leave out one or two characters of a marginal word (e.g. ``the balanc'' instead of ``the balance''). Such 

    As mentioned in the beginning of this section, the parser diverges in a few ways from the corpus annotation methodology when it comes to punctuation marks and treatment of conjunctions. Unfortunately there was no time to create a true golden corpus but OCD and OE corpora are close to that. 

    In the OCD and OE corpus annotations, punctuation marks such as commas, semicolons, three dots and full stops are not included in the constituent segments while the parser includes them at the end of each adjacent segment. 
    
    \begin{figure}[!ht]
        \centering
        \begin{subfigure}[b]{0.47\textwidth}
            \centering
            \begin{tikzpicture}[pattern-node]
            \node[pattern-node] (start) {};
            \node[pattern-node, right = 5em of start] (end) {};
            \draw[edge-style] (start) -- (end) node[midway, above]{conjunct 1};
            
            \node[pattern-node, right = .5em of end] (conj) {and};
            
            \node[pattern-node, right = .5em of conj] (start1) {};
            \node[pattern-node, right = 5em of start1] (end1) {};
            \draw[edge-style] (start1) -- (end1) node[midway, above]{conjunct 2};
            
            \end{tikzpicture}
            \caption{Conjuncts annotated as parallel segments}
            \label{fig:segment-conjunction-paralel}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.47\textwidth}
            \centering
            \begin{tikzpicture}[pattern-node] 
            \node[pattern-node] (start) {};
            \node[pattern-node, right =14em of start] (end) {};
            \draw[edge-style] (start) -- (end) node[midway, above]{conjunct 1};
            
            
            \node[pattern-node, below = 0.1em of end] (end1) {};
            \node[pattern-node, left = 8em of end1] (start1) {};
            \draw[edge-style] (start1) -- (end1) node[midway, above]{conjunct 2};
            
            \node[pattern-node, below = -0.6em of start1, xshift=1.8em] (conj) {and};        
            \end{tikzpicture}
            \caption{Conjuncts annotated as subsumed segments}
            \label{fig:segment-conjunction-subsumed}
        \end{subfigure}
        \caption{Treatment of conjunctions in the corpus compared to the parser}
        \label{fig:conjunction-treatment}
    \end{figure}
    
    The treatment of conjunctions that was discussed in Section \ref{sec:coordination} differs as well. In the corpus, the conjunctions (such as ``and'', ``but'', ``so'', etc.) are excluded from the conjunct segments; they are considered markers in the clause/group complexes rather than part of the constituent. The parser, on the other hand, includes the conjunctions in the following adjacent segment. For example in the corpus there we find segment ``forced me into treatment'' while the parser produces a slightly larger segment ``and forced me into treatment.'' that includes the conjunction at the beginning and the full-stop at the end.
    
    Moreover the conjunct segment spans differ as well due to difference in treatment. Instead of being analysed in parallel, having sibling status as depicted in Figure \ref{fig:segment-conjunction-paralel}, the parse generated conjunct segments are subsumed in a cascade from the former to the latter as depicted in Figure \ref{fig:segment-conjunction-subsumed}.
    
    In this section were mentioned the most important aspects in which the corpora annotations and parser output diverge. The evaluation methodology has been developed to take segmentation discrepancies into consideration in order to provide a wider evaluation ground to the systemic associations. Next section introduces the evaluation methodology.
    
\section{Evaluation methodology}
\label{sec:evaluation-methodology}

\subsection{Reading the corpus segments as a set of mono labelled segments}

    To compare the segment boundaries we need to understand how they are represented in each output and how they can be brought to a common form comparison. 
    All datasets were created with the UAM Corpus Tool \citep{ODonnell2008,ODonnell2008a} version 2.4. The annotations, in this software, are recorded as segments spanning from a start to an end position in the text file together with the set of features (selected from a systemic network) attributed to that segment. There are no constituency or dependency relations between segments. The XML representation of an example annotation segment is provided in Listing \ref{lst:segment1}. There the \textit{id} attribute indicates the unique identification number within the annotation dataset, the \textit{start} and \textit{end} attributes define the segment between two character offsets relative to the beginning of the text file.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=XML,basicstyle=\small\tt,frame=single,caption=Segment example in UAM corpus tool,label=lst:segment1]
<segment id="4" start="20" end="27" 
features="configuration;relational;attributive" 
state="active"/>
\end{lstlisting}
\end{minipage}

     In the current evaluation, the segments are constrained to carry only one label each. The consequence is that segments with multiple features (Figure \ref{fig:segment-multiple}) are broken down into multiple segments with the same span (Figure \ref{fig:segment-simple}) for each feature in the original segment. Doing so permits the evaluation to focus on one or a set of features by selecting only the segments that contain exactly those features. 

    \begin{figure}[!ht]
        \centering
        \begin{subfigure}[b]{0.47\textwidth}
            \centering
            \begin{tikzpicture}[pattern-node]
            \node[pattern-node] (start) {20};
            \node[pattern-node, right = 7em of start] (end) {27};
            \draw[edge-style] (start) -- (end) node[midway, above]{configuration,\\relational,\\attributive};
            \end{tikzpicture}
            \caption{A segment with a set of features}
            \label{fig:segment-multiple}
        \end{subfigure}
        \begin{subfigure}[b]{0.47\textwidth}
            \centering
            \begin{tikzpicture}[pattern-node] 
            \node[pattern-node] (start1) {20};
            \node[pattern-node, right = 7em of start1] (end1) {27};
            \draw[edge-style] (start1) -- (end1) node[midway, above]{configuration};
            
            \node[pattern-node, below = 1em of start1] (start2) {20};
            \node[pattern-node, right = 7em of start2] (end2) {27};
            \draw[edge-style] (start2) -- (end2) node[midway, above]{relational};
            
            \node[pattern-node, below = 1em of start2] (start3) {20};
            \node[pattern-node, right = 7em of start3] (end3) {27};
            \draw[edge-style] (start3) -- (end3) node[midway, above]{attributive};	
            \end{tikzpicture}
            \caption{A set of segments with single features}
            \label{fig:segment-simple}
        \end{subfigure}
        \caption{Example of breaking down a segment with multiple features into set of segments with a single feature}
        \label{fig:segment-breackdown}
    \end{figure}

% todo conclude the section 


\subsection{Turning parser output into a set of mono labelled segments}
% todo update the section start
    
    In order to compare the parser generated output to the corpus segments they need to be turned into the same form. In this section I describe the task of turning rich constituency graphs (CG) into labelled segments similar to those in the corpus. 
    
    Once the parser receives a text as an input, it normalises and segments the text first before performing anything else. Corpus annotation is performed on the raw non-normalised text. To make the parser output segments comparable to the ones in the corpus they need to refer, in terms of their offsets and indexes, to the same raw text. Before the evaluation can take place the parser output segments need to be re-indexed to correspond to the input raw text. 
    %This sections explains the process how the output segmentation is remapped onto the original text.
    
    To fulfil this task, the text processed by the parser is re-indexed back into the original raw text at the level of words (tokens), constituents and sentences. Algorithm \ref{alg:re-index-text} provides pseudo-code of the indexing process.

    \begin{algorithm}[!ht]
        \Input {CG bundle, \text} %, \dg
        \Begin {
            offset $\leftarrow$ 0\;
            \For{\cg \KwTo CG bundle}
            {
                generate segments for \cg indexed on \text given the offset\;
                offset $\leftarrow$ the end of \cg\;
            }
        }
        \caption{Sentence level re-indexing of CG according to the raw text}
        \label{alg:re-index-text}
    \end{algorithm}

    In Section \ref{sec:creation-constituency-graph} was explained that the parser processes one sentence at the time. If more than one sentence is provided as input text the output is then a bundle of constituency graphs. The input for Algorithm \ref{alg:re-index-text} is the array of CGs produced by the parser and the original text. The result of this algorithm is a set of segments indexed according to the raw text. The task is performed by iterating the resulting constituency graphs one by one and indexing each with respect to the offset given by the previous one. The indexing of the CG structure is presented in Algorithm \ref{alg:re-index-words-and-cg}.

    \begin{algorithm}[!ht]
        \Input {\cg, \text, sentence offset} %, \dg
        \Begin {
            words $\leftarrow$ get \cg the list of words \;
            \For{\word \KwTo list of sentence word segments}
            {
                find the \word in the \text after a given sentence offset\;
                \eIf{\word found}
                {
                    start $\leftarrow$ get first word start index\;
                    end $\leftarrow$ get the last word end index\;
                    create a new segment (start, end, \word)\;                
                }
                {
                    generate a warning (manual adjustment needed)\;
                }
            }
            \For{\node \KwTo \cg in BFS postorder}
            {
                find the word span of the constituent\;
                start $\leftarrow$ get first word start index\;
                end $\leftarrow$ get the last word end index\;
                labels $\leftarrow$ get \node class, function and features\;
                create new segment (start, end, labels)\;
            }
            \Return set of segments\;
        }
        \caption{Constituent level re-indexing at the level of constituents according to the raw text}
        \label{alg:re-index-words-and-cg}
    \end{algorithm}

    The way each CG is re-indexed is described by Algorithm \ref{alg:re-index-words-and-cg}. The returned result is a set of segments from the constituency graph considering a given offset. The indexing task is performed first at the word (token) level and the corresponding segments are generated. Then for each constituent node in the CG, segments are generated based on the constituent word span which have already been re-indexed. The indexes of the constituent segments are set to be the beginning of the first word and the end of the last word. The labels assigned to the segments are the constituent unit class, function(s) and all the systemic features. As the segments can carry a single label only then for every feature, function and unit class a new segment is created. This is in line with the practice described above and contributes to clear evaluation methodology.

    Once the parser generated output is re-indexed according to the raw text and represented as a set of mono labelled segments, we can compare this output to the corpus annotations. The next section explains how this is done. 

\subsection{Alignment method and evaluation data}
    
    Both the corpus annotations and the parser output can be represented as a set of mono labelled segments on the raw corpus text. Once they are expressed in this form, we can compare the parser output to the corpus segments and evaluate its accuracy. This section explains how this comparison is done. I first present a strict method of evaluation and then introduce a permissive method of evaluation based on segment similarity. 

    First and straight forwards evaluation method is checking for a perfect match between every segment in the parser output and a segment in the corpus annotations. A perfect match would mean that given a parser segment there exists a corpus segment whose start index, end index and label are the same. 
    
    Using this method of evaluation we can count (a) how many segments with the same label match, (b) how many corpus segments are not matched and (c) how many parser segments are left unmatched. This way we get three count numbers per label for all labels used in the corpus annotation and parser output combined.
    
    In the Section \ref{sec:differences} I presented some differences between the parser output and the corpus annotations. Most of these differences are comparable especially that they manifest as slight variations in the segment spans, i.e. shifted start and/or end segment index, while the segment labels are exactly the same. 
    
    Accounting for differences in the segment spans is a well known task in the mainstream computational linguistics called \textit{text segmentation evaluation}. A variety of segmentation evaluation metrics have been proposed among which the most known are $P_k$ \citep[198--200]{beeferman1999statistical}, \textit{WindowDiff} \citep[10]{pevzner2002critique}, \textit{Segmentation Similarity} \citep[154-156]{fournier2012segmentation} and \textit{Boundary Edit Distance} \citep{fournier2013evaluating}. Each of these metrics have been shown to have some flaws: both $P_k and WindowssDiff$ under-penalise errors \citep{lamprier2007evaluation} and have a bias towards favouring segmentation with few or tightly-clustered boundaries \citep{niekrasz2010unbiased} while segmentation similarity tends to overly optimistic values due to its normalisation \citep{fournier2013evaluating}. 
    
    % All the above metrics take into account that the content of the segment text may vary and so they use text edit distance to account for that. In the case of the current evaluation the segments are defined on the same text and thus only the segment indexes are relevant. 
    
    A simple metric for the difference between the segments taking into account their start and end indexes is that of \textit{geometric distance}. For two segments $S(start_S,end_S)$ and $T(start_T,end_T)$ the geometric distance is defined in Equation \ref{eq:distance}. We can replace the difference between start and end indexes with $\varDelta_{start}$ and $\varDelta_{end}$ notation and obtain the reduced form provided in Equation \ref{eq:distance-simpliefied}. These distances will be discussed in Section \ref{sec:segmentation-evaluation} based on the evaluation data.
    
    \begin{equation} \label{eq:distance}
    d= \sqrt{(start_S - start_T)^{2}+(end_S-end_T)^{2}}
    \end{equation}
    
    \begin{equation} \label{eq:distance-simpliefied}
    d= \sqrt{\varDelta_{start} ^{2}+\varDelta_{end}^{2}}
    \end{equation}

    Now that the metric for comparing segments have been introduced I move on to present the second evaluation method, which, in addition to accounting for the exact matches, accounts for close matches between segments. 
    
    Taking into account the partially matching segments brings us to the second evaluation method. In this case, the task is that of aligning two sets of labelled segments, which is almost the same as the well know problem in computer science called \textit{stable marriage problem} \citep{Gusfield1989}. I adopt onward this frame to explain the evaluation method.
    
    The standard enunciation of the stable marriage problem is provided below and is solved in an efficient algorithm named Gale-Shapley \citep{Gale1962} after its authors.
    
    \begin{quotation}
        Given \textit{n} men and \textit{n} women, where each person has ranked all members of the opposite sex in order of preference, marry the men and women together such that there are no two people of opposite sex who would both rather have each other than their current partners. When there are no such pairs of people, the set of marriages is deemed stable \citet{iwama2008}.
    %    \footnote{see \href{https://en.wikipedia.org/wiki/Stable_marriage_problem}{stable marriage problem on Wikipedia}}
    \end{quotation}

    In the context of this evaluation the group of men is associated with the segments generated automatically by the parser and the group of women with the segments available from the manual analysis. 

    The standard stable marriage problem is formulated such that there is a group of men and a group of women and each individual from each group expresses their preferences for every individual from the opposite group as an ordered list. The assumption is that the preferences of every individual are known and expressed as a complete ordered list of individuals from the opposite group ranging from the most to the least preferred one. Thus the preference list must be \textit{complete} and \textit{fully ordered}. 

    To fulfil these requirements I construct a distance matrix from each automatically created segment to every manually created one. The distance measure considered here is the Euclidean one provided in Equation \ref{eq:distance-simpliefied} above. The matrix represents the complete and fully ordered set of preferences stipulated in the original problem formulation. In addition to having identical offsets, the segments need to carry the same labels in order to be considered a match. This condition is not expressed in the original problem but is considered in Algorithm \ref{alg:matching}. 

    \begin{algorithm}[!ht]
        \Input{\aslist, \mslist} %, \dg
        \Begin{
            mark all \aslist and \mslist free\;
            compute distances from each \mslist to every \aslist\;
            \While{$\exists$ free \aslist}{
                \as $\leftarrow$ first free from \aslist\;
               \If{$\exists$ \mslist not yet tested to match \as}{
                   \ms $\leftarrow$ the nearest among \mslist to \as with identical label \;
                   \If{\ms is free}{
                       match \as and \ms \;
                       mark \as and \ms as non-free \;
                   }               
               \Else{
                       $\as'$ $\leftarrow$ the current match of \ms \;
                       \If{\as is closer to \ms than $\as'$}{
                           match \as and \ms \;
                           mark \as and \ms as non-free \;
                           mark $\as'$ as free \; 
                       }
                   }
               }
               \Else{
                   mark \as as non-free and non-matching \;
               }
            }
        }
        \caption{The algorithm for matching automatic and manual segments}
        \label{alg:matching}
    \end{algorithm}

    Using the second method of evaluation, presented above, we can count for every distinct label (a) how many segments match perfectly, i.e. the distance is zero, (b) how many segments partially match, i.e. the distance is greater than zero, (c) how many corpus segments are unmatched and (d) how many parser segments are unmatched. This way we get a four count numbers per label for all labels used in the corpus annotation and parser output combined that we can use to compute parser accuracy. Further more the partial matches can be analysed to estimate the degree of the deviation and derive insights what can be done about it. Having said that I further proceed with presenting the evaluation data. 

\section{Evaluation of the syntactic structure}
\label{sec:syntactic-evaluation}
    In this section I present ...
    
\subsection{Segmentation evaluation}
\label{sec:segmentation-evaluation}
    
    This section presents the evaluation data on text segmentation. As we will see below, most of the parser segments coincide with the corpus segments, but not all of them. The differences, which were described in Section \ref{sec:differences}, are mainly due to minor differences in annotation approach, text normalisation and trimming before parsing (that was not performed before the manual annotation), errors in the annotations (missing or including extra characters). 
    
    The segmentation counts are provided in Table \ref{tab:segmentation-stats}. The columns represent  the number of matched segments, segments from the corpus that have not been matched and the parser output segments left unmatched.
    
    \begin{table}[!ht]
    \centering
    \begin{tabular}{lccc}
    \toprule
    {} &  Matched &  Corpus non-matched &  Parser non-matched \\
    \midrule
    Exact matches only &     6665 &                1319 &                4332 \\
    Exact and close matches &    11073 &                1319 &                4332 \\
    \bottomrule
    \end{tabular}
    \caption{Count statistics of matched and non-matched segments}
    \label{tab:segmentation-stats}
    \end{table}
    
    There are over 6500 segments from the corpus and the parser output that perfectly coincide. In total there are over 11000 segments that coincide partially or completely. This means that the exact matched are included into the latter along with other 5000 partially matching segments. 
    
    \begin{table}[!ht]
    \centering
    \begin{tabular}{lccc}
    \toprule
    {} &  Precision &  Recall &   F1 \\
    \midrule
    Exact matches only &        0.61 &    0.83 & 0.70 \\
    Exact and close matches                &        0.72 &    0.89 & 0.80 \\
    \bottomrule
    \end{tabular}
    \caption{Segmentation accuracy}
    \label{tab:segmentation-accuracy}
    \end{table}
    
    The statistics provided in Table \ref{tab:segmentation-stats} translate into precision, recall and $F_1$ scores as provided in Table \ref{tab:segmentation-accuracy}. We can see that all the values increase when we take the partial matches into consideration, jumping from 0.71 to a 0.8 $F_1$ score. The close (partial) matches between segments are measured in several ways. An analysis of how these distances are distributes follows below. 
    
    The segmentation differences are measured, as introduced in Section \ref{sec:evaluation-methodology}, using a few distance metrics: (a) geometric (Euclidean) distance, (b) edit (Levenshtein), (c) generalised hamming distance (GHD) \citep{Bookstein2002}, $P_k$ \citep[198--200]{beeferman1999statistical}, \textit{WindowDiff} \citep[10]{pevzner2002critique}. 
    The data are calculated on a number of over 12500 segment pairs out of which 62\% are exact matches and 38\% are close matches. 
    
    Before providing the interpretations to the data I first show that two of these distances are strongly correlated to each other and thus can be omitted from the discussion. Table \ref{tab:correlation-matrix-distances} represents correlation matrix for the distance types. 
    
    \begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabulary}{\textwidth}{lccCcc}
    \toprule
    {} &  Levinstein &  Geometric &  Generalised Hamming &   Pk &  WindowDiff \\
    \midrule
    Levinstein          &        1.00 &       0.99 &                 0.53 & 0.45 &        0.54 \\
    Geometric           &        0.99 &       1.00 &                 0.52 & 0.45 &        0.54 \\
    Generalised Hamming &        0.53 &       0.52 &                 1.00 & 0.84 &        0.92 \\
    Pk                  &        0.45 &       0.45 &                 0.84 & 1.00 &        0.92 \\
    WindowDiff          &        0.54 &       0.54 &                 0.92 & 0.92 &        1.00 \\
    \bottomrule
    \end{tabulary}
    }
    \caption{Pearson correlation coefficients for pairs of distance measure types}
    \label{tab:correlation-matrix-distances}
    \end{table}
    
    I employ Pearson correlation coefficient which measures the direction and strength of a linear correlation between two variables, in this case pairs of distance types. The standard interpretation for this coefficient is as follows. A coefficient value between 0.1 and 0.3 indicated a weak linear relationship between the variables. If it is between 0.3 and 0.5 the relation is moderate while between 0.5 and 0.7 it indicates a strong; and between 0.7 and 1 it represents a very strong correlation of variables. 
    
    Using this rule of thumb we can say that there Levinstein and Geometric distances are almost the same. At the same both are only moderately related to the other three distance types. From this point on I will exclude the Levinstein distance and employ the geometric distance only as representative for both. 
    
    The generalised Hamming distance, $P_k$ and WindowDiff bear a strong correlation to each other. $P_k$ and WindowDiff are strongly correlated to each other and so I decide to exclude $P_k$ and generalised hamming distance from further discussions and use WindowDiff as the representative of the three distances. This choice is based on the fact that WindowDiff was proposed to overcome weaknesses of $P_k$ \citep[10]{pevzner2002critique}.
    
    I have shown that Geometric distance and WindowDiff distance are the representative distance measures in his evaluation that only mildly correlate to each other. Next I provide interpretations the descriptive data provided in Table \ref{tab:distance-descriptions} for each of them.
    
    \begin{table}[!ht]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabulary}{\textwidth}{LccccRcc}
    \toprule
    {} &  Min &    Max &  Mean &   Std &  Relative std &  Skew &  Kurtosis \\
    \midrule
    Levinstein          & 0.00 & 219.00 &  5.37 & 15.99 &          2.98 &  5.57 &     42.63 \\
    \textbf{Geometric}           & 0.00 & 219.00 &  4.99 & 14.67 &          2.94 &  5.56 &     43.14 \\
    Generalised Hamming & 0.00 &   8.00 &  1.84 &  2.87 &          1.56 &  1.30 &      0.09 \\
    Pk                  & 0.00 &   1.00 &  0.18 &  0.29 &          1.62 &  1.40 &      0.57 \\
    \textbf{WindowDiff}          & 0.00 &   0.86 &  0.15 &  0.23 &          1.59 &  1.32 &      0.28 \\
    \bottomrule
    \end{tabulary}
    }
    \caption{Descriptive statistics for each series of distance measurements between corpus and parser segments}
    \label{tab:distance-descriptions}
    \end{table}

    Table \ref{tab:distance-descriptions} presents the descriptive statistics for every distance type. The \textit{std} column means \textit{standard deviation} ($\sigma$) while the \textit{relative std} represents \textit{relative standard deviation} (or coefficient of variation) which is the ration between the standard deviation and mean value ($\mu$), i.e. ($\sigma/\mu$). It measures how concentrated the data are around the mean, the more concentrated, the smaller the standard deviation. It is considered that a relative standard deviation between 0 and 0.5 indicates tightly clustered data around the mean; if it is situated between 0.5 and 1 then it means the data are more spread out; if, however the value is over one then it means the data are very scattered. 
    
    Besides the mean and (relative) standard deviation, \textit{Skew} and \textit{Kurtosis} are other statistical indicators describing a distribution. Skewness measures the asymmetry of the bell of the normal distribution where skewness greater than 1 indicates that the data are highly skewed to the right, i.e that there are rare data points situated in a long tail. Kurtosis measures the outliers present in the distribution. Kurtosis smaller than the threshold of 3 indicates that the data has light tails or lack of outliers, whereas a value greater than 3 indicates heavy tail and requires additional investigation as it may indicate among others wrong data. 

    \begin{figure}[!ht]
    \centering
    \includegraphics[width=.85\textwidth]{evaluation-results/figures/distance-distribution-histogram-Geometric-25.pdf}
    \caption{Matched segments geometric distance distribution histogram (binning=25)}
    \label{fig:distance-distribution-histogram-Geometric-25}
    \end{figure}

    In the current evaluation, the geometric distance between corpus and parser segments spans from a minimum 0 to maximum 219 characters. The mean distance is 4.99, which is close to the minimum point, with a standard deviation of 14.67, which, in relative terms, indicates an extreme deviation of 2940\%. The skew over 1 indicates a strong asymmetry to the right, and the kurtosis of 43.14 (almost 15 times the threshold of 3) indicates that most of the data, about 80\%, gravitate towards the left, between 0 and slightly over the mean, while the rest of the data point continue into a very long tail to the right. This is depicted in Figure \ref{fig:distance-distribution-histogram-Geometric-25}. 
    
    As appears in Figure \ref{fig:distance-distribution-histogram-Geometric-25}, the data follow a power law distribution \citep{newman2005power}. Here 62\% are not shifted at all and 83\% of the segments are slightly shifted up to 5 characters. There rest (27\%) of the segments are shifted by more than 5 characters. These ratios approximate Patetto's 80-20 distribution law but may as well fit Zipf's law \citep{newman2005power}. In future work, properties of these data should further be analysed, including the distribution fitting, that is selection of the theoretical distribution that fits best to a dataset.
    
    \begin{figure}[!ht]
    \centering
    \includegraphics[width=.85\textwidth]{evaluation-results/figures/distance-distribution-histogram-WindowDiff-25.pdf}
    \caption{Matched segments WindowDiff distance distribution histogram (binning=25)}
    \label{fig:distance-distribution-histogram-WindowDiff-25}
    \end{figure}
    
    Next distance that I discuss is WindowDiff distance between corpus and parser segments. One crucial difference to geometric distance is the normalisation to [0,1] interval. In the current evaluation, the WindowDiff distance distribution, depicted in Figure \ref{fig:distance-distribution-histogram-WindowDiff-25}, spans from a minimum 0 to maximum score of 0.86. The mean distance is 0.15 with a standard deviation of 0.23 i.e. $\pm$159\%. The mean value is close to the minimum point, the relative standard deviation indicated extreme deviations of 159\%, the skew over 1 indicates a strong asymmetry to the right, and the kurtosis of 0.28 indicate the distribution does not have many outliers in the tail. 
    
    One positive aspect of this distance distribution is that the tail is not so long due to its normalised structure. This permits aggregation of the outliers we have observed in geometric distance distribution into a compact spectrum. This way, kurtosis, which, in this case, is smaller than 3, no longer indicates an abnormally long tail of outliers. 
    
    The histogram in Figure \ref{fig:distance-distribution-histogram-WindowDiff-25} resembles a power law distribution and more analysis work needs to be done in the future, including the distribution fitting and relation to the causes of partial matches in the first place. 
    
    This section presented the segmentation evaluation and distance analysis of the partially matching segments. The data show that the parser generates segments exactly as provided in the corpus with an accuracy of 0.71, and segments that partially correspond to those in the corpus with an accuracy of 0.8. We will see in the next sections that this partial match is supported by the identical labels that the segments carry. The distances of the partially matched segments, in about 80\% of the cases do not exceed 5 characters, but can span, most probably by mistake, over 200 characters. Next I move on to present the evaluation of the segment label assignments, which in our case are unit classes and functions.

\subsection{Unit class evaluation}
\label{sec:unit-class-evaluation}
    In this and the next section I present the parser syntactic accuracy. The syntactic accuracy aims to measure how well the main unit types and the clause main elements have been detected by the parser compared to the corpus. The evaluation is performed on the OCD corpus. This evaluation is restricted to the clause and four group types: nominal, prepositional, adverbial and adjectival. No clause complexes, group complexes or word types are included. The evaluation data are depicted in Figure \ref{fig:unit-types-data}. The names of the unit classes are provided on the x axis at the bottom of the graph while on the y axis the absolute number of occurrences is provided. 
    
    The meaning of exact and close match has been explained in Section \ref{sec:segmentation-evaluation} and from now on the label ``Matched'' will mean the segments that are either exactly or closely matched all together, while the label with a remark ``(exact only)'' means that it applies to only the portion of the exactly matched segments. 
    
    
    \begin{figure}[!ht]
    \centering
    \includegraphics[width=.85\textwidth]{evaluation-results/figures/unit-types-data.pdf}
    \caption{Bar chart of matched and non-matched segments for the main unit classes}
    \label{fig:unit-types-data}
    \end{figure}
    
    To make the data easier to read and interpret I present the evaluation data in a table form using relative values to the number of matched segments. The absolute values of the evaluation statistics are contained in the graphical form and also available in the appendices in the table form. The relative evaluation data in this and the following sections will be presented in tables with the same structure. Using Table \ref{tab:unit-types-relative} as example, the column meaning is as follows. The first column contains the name of the unit class, element or feature. The column ``Matched'' contains the absolute number of matched segments with a specific label. The three other columns represent the number of segments relative to the ``Matched'' ones. So the column ``(\%) Matched (exactly only)'' means that out of all the matched segments that many represent exact matches and the rest to 100\% are partially matched segments. The column ``(\%) Corpus non-matched'' represent the number of segments relative to the total number of segments of particular type in the corpus, which remain unmatched. The column signifies the fraction of segments that remain unmatched, while the rest up to 100\% have, each, a corresponded in the parser output. The column ``(\%) Parser non-matched'' represents the number of segments (relative to the total number of segments of particular type in the parser output) in the parser output that do not have a correspondent in the corpus.
    
    \begin{table}[!ht]
    \centering
    \begin{tabulary}{\textwidth}{lCCCC}
    \toprule
    {} &  Matched &  (\%) Matched (exactly only) &  (\%) Corpus non-matched &  (\%) Parser non-matched \\
    \midrule
    nominal-group       &   564.00 &                       70.39 &                   12.96 &                    9.47 \\
    clause              &   477.00 &                       13.84 &                    9.83 &                   12.64 \\
    adverbial-group     &   131.00 &                       76.34 &                   29.95 &                   40.18 \\
    prepositional-group &    90.00 &                       41.11 &                   25.00 &                   27.42 \\
    adjectival-group    &    33.00 &                       24.24 &                   49.23 &                   44.07 \\
    \bottomrule
    \end{tabulary}
    \caption{The evaluation statistics relative to the number of matched segments for the main unit classes}
    \label{tab:unit-types-relative}
    \end{table}
    
    The evaluation data from Table \ref{tab:unit-types-relative} indicate that most (over 70\%) of the nominal and adverbial groups are identified with exact same borders as in the corpus while clause borders exhibit the most disagreement reflected by their low score of exact matches, only 13.84\%. The proportion of unmatched unit class segments in both the corpus and parser output varies between 9\% for clauses and nominal groups, and over 40\% for adjectival and adverbial groups. These proportions, however, are better interpreted when they are embedded into precision and recall score, which are provided in Table  \ref{tab:unit-types-combined-F1}. 
    \begin{table}[!ht]
    \centering
    \begin{tabular}{lcccccc}
    \toprule
     & \multicolumn{3}{c}{Exact match only} & \multicolumn{3}{c}{Exact and close match} \\ \cline{2-7} 
     & Precision & Recall & F1 & Precision & Recall & F1 \\ 
    \midrule
    nominal-group & 0.87 & 0.83 & 0.85 & 0.91 & 0.87 & 0.89 \\
    adverbial-group & 0.53 & 0.64 & 0.58 & 0.87 & 0.90 & 0.89 \\
    prepositional-group & 0.52 & 0.55 & 0.54 & 0.73 & 0.75 & 0.74 \\
    clause & 0.49 & 0.56 & 0.52 & 0.60 & 0.70 & 0.65 \\
    adjectival-group & 0.24 & 0.20 & 0.22 & 0.56 & 0.51 & 0.53 \\ 
    \bottomrule
    \end{tabular}
    \caption{Parser accuracy statistics for for the main unit classes}
    \label{tab:unit-types-combined-F1}
    \end{table}
    
    The scores provided in the first three columns are calculated with respect to exact matches only, which can be seen reflected in the lower precision, recall and $F_1$ scores when compared to their correspondents in the last three columns. These scores, nonetheless, constitute an appropriate baseline for comparing parser accuracy for when the close matches are considered as well. Note that in all cases of a match, close or exact, the segments bear the same label, and so, as explained in Section \ref{sec:differences}, the source of the divergence is in the segmentation. 
    
    The last three columns in Table \ref{tab:unit-types-combined-F1} show that clause and normal group units are identified with almost 0.9 $F_1$ measure, which is an encouraging result, while the adjectival and adverbial groups score 0.53 and 0.65 indicates that there is some space for improvement. 
    Further investigation is needed to discover the reason for the lower scores as there seem to be no obvious cause other than corpus and/or parser errors. Also, as visible in Figure \ref{fig:unit-types-data}, there is a contrast in the number of segments between the first two unit types and the last three with a ratio of one to four or more. The low number of exemplars in this evaluation contributes to a certain extent to the lower accuracy statistics.
    
\subsection{Clause Mood elements evaluation}
\label{sec:unit-mood-element-evaluation}

    In this section I describe the evaluation statistics reflecting the parser capacity to detect the main elements of a clause. The data available in the corpus unfortunately does not permit to evaluate units of lower rank such as nominal, prepositional, adjectival and other groups. Next I present statistics on the clause Mood elements, which were described in Chapter \ref{ch:the-grammar}. These elements are present in the annotations of the OCD corpus as explained in the beginning of this chapter in Section \ref{sec:corpus}. 
    
    \begin{figure}[!ht]
    \centering
    \includegraphics[width=.85\textwidth]{evaluation-results/figures/unit-elements-mood-data.pdf}
    \caption{Bar chart of matched and non-matched segments for the clause main Mood elements}
    \label{fig:unit-elements-mood-data}
    \end{figure}
    
    The OCD corpus annotations provide with the main syntactic (Mood) elements in the clause. Some of them, such as Auxiliary verbs, Main verb extension, Negation particle, and others have been omitted in the corpus and are thus missing in the present evaluation. Figure  \ref{fig:unit-elements-mood-data} reflects the absolute values from the empirical data. 
    
    The parser accuracy measurements for syntactic are contained in Table \ref{tab:unit-elements-mood-combined-F1}. The $F_1$ score for subjects and main verbs is nearly 0.9 while the complements and adjuncts are over 0.6. Finite element scores nearly 0.5 which is a low score for such easy to detect element. The reason for it lays in the incomplete corpus annotations, which fails to account for conflation of the finite and main verb elements. So when there was a main verb that was also a finite, only the main verb function was marked, which is incomplete. This incompleteness is also reflected in the discrepancy between low precision of 0.32 and high recall of 0.98. 
    
    \begin{table}[!ht]
    \centering
    \begin{tabular}{lcccccc}
    \toprule
     & \multicolumn{3}{c}{Exact match only} & \multicolumn{3}{c}{Exact and close match} \\ \cline{2-7} 
     & Precision & Recall & F1 & Precision & Recall & F1 \\ 
    \midrule
    main verb & 0.86 & 0.90 & 0.88 & 0.87 & 0.90 & 0.89 \\
    subject & 0.82 & 0.94 & 0.88 & 0.84 & 0.95 & 0.89 \\
    complement & 0.27 & 0.73 & 0.39 & 0.53 & 0.89 & 0.66 \\
    adjunct & 0.50 & 0.62 & 0.55 & 0.58 & 0.69 & 0.63 \\
    finite & 0.32 & 0.98 & 0.48 & 0.32 & 0.98 & 0.49 \\
    \bottomrule
    \end{tabular}
    \caption{Parser accuracy statistics for the clause main Mood elements}
    \label{tab:unit-elements-mood-combined-F1}
    \end{table}
    
    The number of complements unmatched in the parser output is nearly the same as the number of matched complements. This is reflected in the 0.53 precision score and nearly 0.9 recall rate which overall lead to an $F_1$ score lower than that of subject and main verb elements. This can be explained by a flaw, mentioned in Section \ref{sec:corpus}, in the annotation methodology as follows. The clausal complements often were annotated as a new clauses omitting to draw the same segment and marking it to be a complement in the clause above. This required the corpus revision and correction. Adjuncts however have a higher number of unmatched segments on both sides and this may be due to bugs in the parser and other mistake or omissions in the corpus.
    
\subsection{Clause Transitivity elements evaluation}
\label{sec:unit-transitivity-element-evaluation}

    OE corpus, provides with elements needed in semantic (Transitivity) parsing. The employed elements are Configuration, Participant role and Main verb while Circumstances are excluded from the study. Figure \ref{fig:unit-elements-transitivity-data} summarises the evaluation data. 
    
    The configuration segments, in SFG, correspond to clause segments, the participant role segments have as correspondents either the subject or complement segments, while the Main verb segments are shared. The aggregation of Subjects and Complements can be observed in Figure \ref{fig:unit-elements-transitivity-data} where the number of participant roles is approximately double the number of configurations. A configuration can have between one and three participants, and current data show an average of two participants per clause. 
    
    \begin{figure}[!ht]
    \centering
    \includegraphics[width=.85\textwidth]{evaluation-results/figures/unit-elements-transitivity-data.pdf}
    \caption{Bar chart of matched and non-matched segments for the clause main Transitivity elements}
    \label{fig:unit-elements-transitivity-data}
    \end{figure}
    
    Note that in Figure \ref{fig:unit-elements-transitivity-data} the scale stretches over 2500 which reflects a much larger number of segments in OE corpus than that available in the OCD corpus. This, and certainly a higher quality of annotations, is reflected in the fairly uniform evaluation results. The $F_1$ (0.82), precision (0.74) and recall (0.92) scores vary very little across elements when compared to scores of the syntactic elements shown in Table \ref{tab:unit-elements-mood-combined-F1},  which vary substantially from one element to another.   

    \begin{table}[!ht]
    \centering
    \begin{tabular}{lcccccc}
    \toprule
     & \multicolumn{3}{c}{Exact match only} & \multicolumn{3}{c}{Exact and close match} \\ \cline{2-7} 
     & Precision & Recall & F1 & Precision & Recall & F1 \\ 
    \midrule
    participant-role &       0.62 &    0.88 & 0.73 &       0.74 &    0.92 & 0.82 \\
    configuration    &       0.22 &    0.52 & 0.30 &       0.74 &    0.92 & 0.82 \\
    main verb        &       0.62 &    0.86 & 0.72 &       0.71 &    0.90 & 0.79 \\ 
    \bottomrule
    \end{tabular}
    \caption{Parser accuracy statistics for the clause main Mood elements}
    \label{tab:unit-elements-transitivity-combined-F1}
    \end{table}
    
    In case of exact match evaluation the scores are lower for participant roles and main verbs, situating at 0.73 $F_1$ value, while configuration $F_1$ plummeted to 0.3. As configurations correspond in SFG to clauses boundary establishment methodology plays a significant role in achieving exact matches. Thus the discrepancy in the $F_1$ score between exact and combined matches is explainable by a discrepancy in the clause boundaries establishment and was already addressed in Section \ref{sec:differences}. 
    
    We come to the end of the syntactic structure evaluation where I have shown that the parser segments text exactly as in the corpus with an accuracy of 0.7 and an approximately same segmentation with an accuracy of 0.8. The parser detects unit classes on average with a score of 0.52 $F_1$ for exact matches and 0.74 $F_1$ for close matches. The Mood and Transitivity elements are detected on average with a 0.71 and 0.81 $F_1$ scores. 
    
    A constituency parser that generates a syntactic analysis using comparable unit classes and functions (using phrase structure grammars) such as for example \citet{chen2014fast}, \citet{stern2017minimal} or \citet{kitaev2018multilingual} reach an accuracy of 0.95 $F_1$ for English. This state of the art in parsing with other grammars reflects that there is a large space to improve the accuracy of Parsimonious Vole constituency. But it should not be separated from the main mission of this work to parse with constituency structures enriched with features from system networks. Next I present the evaluation of the the systemic feature selections from the MOOD and from the TRANSITIVITY system networks. 

    
    
\section{Evaluation of the systemic features assignment}
\label{sec:systemic-evaluation}
    In this section ... 
    % todo: 
    
    % todo say that the exact and close matches no longer play a role in the evaluation because it is about the features assigned to the unit and is of paradigmatic nature, thus has nothing to do with the syntagmatic dimension where the segments live :) 
    
\subsection{Evaluation of the MOOD systemic features assignment}
\label{sec:systemic-evaluation-MOOD}

    In this section I present the evaluation results for the systemic selections from the MOOD system network assigned to units in the constituency structure. The fragment of the MOOD system network used in this evaluation was presented as part of the OCD corpus description in Section \ref{sec:corpus}. The parser provides more feature selections as described in Section \ref{sec:mood} of the chapter describing the parser grammar, but the current evaluation is limited to only what is available in the corpus.

    In Table \ref{tab:features-mood} are provided the evaluation results for each of the MOOD features grouped by system network. On average the parser assigns systemic features with a precision of 59\%. I do not address the evaluation of every feature in part but analyse the results as a whole taking a few systems as examples for discussion. Addressing each system in part aiming at understanding why the score is satisfactory of not should be tackled in future work.

     \begin{table}[!ht]
        \centering
        \resizebox{0.8\textwidth}{!}{%
            \begin{tabulary}{1.1\textwidth}{@{}lCCCccc@{}}
            \toprule
             & Match & Corpus non-matched & Parser non-matched & Precision & Recall & F1 \\ \midrule
            POLARITY-TYPE &  &  &  &  &  &  \\
            positive & 485 & 125 & 55 & 0.90 & 0.80 & 0.84 \\
            negative & 57 & 10 & 70 & 0.45 & 0.85 & 0.59 \\
            VOICE-TYPE &  &  &  &  &  &  \\
            active & 553 & 102 & 68 & 0.89 & 0.84 & 0.87 \\
            passive & 11 & 11 & 28 & 0.28 & 0.50 & 0.36 \\
            FINITNESS &  &  &  &  &  &  \\
            non-finite & 99 & 19 & 38 & 0.72 & 0.84 & 0.78 \\
            finite & 526 & 33 & 554 & 0.49 & 0.94 & 0.64 \\
            NON-FINITE-TYPE &  &  &  &  &  &  \\
            perfective & 71 & 12 & 16 & 0.82 & 0.86 & 0.84 \\
            imperfective & 26 & 9 & 24 & 0.52 & 0.74 & 0.61 \\
            DEICTICITY &  &  &  &  &  &  \\
            temporal & 446 & 74 & 55 & 0.89 & 0.86 & 0.87 \\
            modal & 12 & 33 & 6 & 0.67 & 0.27 & 0.38 \\
            MOOD-ASSESSMENT-TYPE &  &  &  &  &  &  \\
            temporality & 35 & 17 & 27 & 0.56 & 0.67 & 0.61 \\
            modality & 15 & 32 & 8 & 0.65 & 0.32 & 0.43 \\
            intensity & 12 & 14 & 43 & 0.22 & 0.46 & 0.30 \\
            MOOD-TYPE &  &  &  &  &  &  \\
            indicative & 455 & 216 & 37 & 0.92 & 0.68 & 0.78 \\
            imperative & 4 & 1 & 31 & 0.11 & 0.80 & 0.20 \\
            INDICATIVE-TYPE &  &  &  &  &  &  \\
            declarative & 355 & 260 & 27 & 0.93 & 0.58 & 0.71 \\
            interrogative & 47 & 7 & 63 & 0.43 & 0.87 & 0.57 \\
            INTERROGATIVE-TYPE &  &  &  &  &  &  \\
            wh & 40 & 6 & 57 & 0.41 & 0.87 & 0.56 \\
            yes-no & 5 & 3 & 8 & 0.38 & 0.62 & 0.48 \\
            WH-SELECTION &  &  &  &  &  &  \\
            wh-subject & 9 & 3 & 7 & 0.56 & 0.75 & 0.64 \\
            wh-adjunct & 11 & 15 & 3 & 0.79 & 0.42 & 0.55 \\
            wh-complement & 8 & 0 & 62 & 0.11 & 1.00 & 0.21 \\ \bottomrule
            \end{tabulary}
        }
        \caption{The evaluation statistics available for the MOOD system network}
        \label{tab:features-mood}
    \end{table}
    
    Table \ref{tab:features-mood} summarises the evaluation results for the MOOD system network features grouped by system name marked with capital letters. The order in which the features appear in the table follow roughly an increase in systemic delicacy. As delicacy increases there are increasingly fewer occurrences where a system is employed. This is also associated also by a decrease in accuracy although there are multiple factors influencing it among which corpus quality and small population size. 
    
    The precision and recall values vary quite a lot from a minimum of 11\% up to a maximum of 93\% and the harmonic mean, the $F_1$ score, between 30\% and 87\%  averaging to almost 60\%. The details can be read in Table \ref{tab:mood-accuracy}. The distribution of these values can be seen in Figures \ref{fig:mood-precission-recall} and \ref{fig:mood-precission-f1}. A noticeable feature is the presence of two peaks in the precision and recall distributions: one around 50\% and the other one around 90\%. They translate into a similar $F_1$ distribution with peaks at 60\% and 85\%, a phenomena which I address next.

    \begin{table}[!ht]
        \centering
        \begin{tabular}{lccc}
            \toprule
            {} & {Precision} & {Recall} & {F1} \\ %\hline
            % feature count & 21.00 & 21.00 & 21.00 \\ \hline
            \midrule
            mean & 0.57 & 0.73 & 0.59 \\ 
            standard deviation & 0.27 & 0.18 & 0.21 \\ 
            min value & 0.11 & 0.32 & 0.20 \\ %\hline
            25\% quantile & 0.41 & 0.62 & 0.48 \\ %\hline
            50\% quantile & 0.56 & 0.80 & 0.61 \\ %\hline
            75\% quantile & 0.82 & 0.86 & 0.78 \\ %\hline
            max value & 0.93 & 1.00 & 0.87 \\ %\hline
            \bottomrule
        \end{tabular}
        \caption{Descriptive statistics of the precision, recall and F1 scores for evaluated MOOD features}
        \label{tab:mood-accuracy}
    \end{table}

    Within most systems, the $F_1$ scores exhibit a contrast from one feature to the other. What this means is discussed in the next two cases. For example in the POLARITY-TYPE system the positive polarity feature scores 84\% $F_1$ measure and negative one almost 60\%. As per Definition \ref{def:system}, the system features are mutually exclusive. The polarity of an English clause is positive by default unless a negation marker is found and this represents only 10\% of the clauses in  the corpus. This means that it should be sufficient for the parser to detect one feature with a reasonably high $F_1$ score then the converse feature should be detectable with the same degree of accuracy. Yet the current data invalidate this hypothesis as the grammar does not represent exclusivity.
    
    \vspace{1em}
    \noindent
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{evaluation-results/figures-old/accuracy-syntactic-mood-precission-recall.png}
        \captionof{figure}{The distribution of precision and recall for selected features from the MOOD system network}
        \label{fig:mood-precission-recall}
    \end{minipage}
    \quad
    \begin{minipage}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{evaluation-results/figures-old/accuracy-syntactic-mood-f1.png}
        \captionof{figure}{The distribution of F1 score for selected features from the MOOD system network}
        \label{fig:mood-precission-f1}
    \end{minipage}
    \vspace{1em}
    
    In the case of the POLARITY-TYPE system, the phenomena may be explained as a consequence of low delicacy in the parsing algorithm. The current implementation determines polarity by checking for the presence of the negation verbal marker only. Nonetheless a more delicate polarity testing will have to take into consideration polarity indicators from the subject, complement and adjuncts of various types that may have been taken into consideration during the annotation process. Providing an incomplete, less delicate implementation for systemic choices may be a source of errors and thus this hypothesis requires further investigation.  %The same can be expected of other systems that are reduced in delicacy 
    
    Nonetheless, the same phenomena can be seen if we look at the systems that do not span others with higher degrees of delicacy. For example, the detection mechanism for VOICE-TYPE is implemented similarly to POLARITY-TYPE. The parser checks whether there is a passive order of elements in the clause, otherwise the active voice is selected.  Detection of the positive voice scores a much higher $F_1$ measure of 87\% than the negative one of 36\%. There is no problem of low delicacy and still there is a discrepancy between the $F_1$ scores of the two features which is somewhat a surprising result. 

    One way to look at the high discrepancy between the feature accuracy scores can be as follows. 
    If we take VOICE-TYPE as example, most instances of active voice are easy to detect. But there is a portion of cases, regardless which voice it is, that are difficult for the parser to distinguish. The passive voice selections are executed mostly for these ambiguous cases. It it possible that this can be generalised to other system networks and more investigation is required to pinpoint exactly how such phenomena materialise.

    In future implementations we can take advantage of the mutual exclusivity of system features and capitalise on the above finding, at least for the leaf systems. Taking into consideration that the score of one feature is higher, clause elements should then be provided with selections of those features only, while the complement features, which score low in this evaluation should remain unassigned. This means that only features that can be provided with high confidence (using $F_1$ score as a measure of that) shall be assigned. Then an additional process can be implemented to select the remaining unassigned features. 
    
    % todo conclude this section

\subsection{Evaluation of the TRANSITIVITY systemic features assignment}
\label{sec:systemic-evaluation-TRANSITIVITY}
    
    In this section I present the evaluation of the TRANSITIVITY system network. As was explained in Section \ref{sec:corpus} above, the OE corpus contains annotations with the part from the original system network depicted in Figure \ref{fig:cardiff-transitivity}. The parser provides more feature selections as described in Section \ref{sec:transitivity} of the chapter describing the parser grammar, but the current evaluation is limited to only what is available in the corpus. 
    
    Transitivity analysis is semantic in nature and poses challenges in meaning selection beyond constituent class or function. The approach of assigning this sort of features was explained in Chapter \ref{ch:semantic-parsing}. Here I would like to remind an important aspect that impacts the evaluation results, explaining in some cases recall being higher than precision.
    
    In the grammar, described in Chapter \ref{ch:the-grammar}, a clause can be assigned a single process configuration and whose participant constituents can take only one role each. In the semantic role labelling task the situation is similar: a clause takes one semantic frame and each constituent only one semantic label. This means that a parser shall generate as output one semantic configuration that fits best the text. 
    
    The Parsimonious Vole parser does not always provide a single semantic configuration. Instead it generates one or several possible configurations for each clause instead of providing exactly a one. The reason for this is the mechanism by which semantic analysis is generated. The constituency structure is tested against a set of semantic graph patterns and the matching patterns enrich the constituency structure with semantic features immediately. In some cases more than one pattern matches the constituency structure leading to enrichment by multiple graph patterns, which is not entirely true. Thus these multiple assignments should be interpreted as alternating possibilities.  
    
    Intuitively, this should reduce recall on all the elements but the effects are mostly manifested at the level of participant roles as will be described below. First let's discuss the evaluation results for the process types, which are provided in Table \ref{tab:features-transitivity}, and after we will turn to the evaluation of participant roles.
    
    \begin{table}[!ht]
    \centering
    \resizebox{0.8\textwidth}{!}{%
        \begin{tabulary}{1.1\textwidth}{@{}lCCCccc@{}}
        \toprule
         & Match & Corpus non-matched & Parser non-matched & Precision & Recall & F1 \\ \midrule
        PROCESS-TYPE &  &  &  &  &  &  \\
        mental & 277 & 231 & 87 & 0.76 & 0.55 & 0.64 \\
        relational & 338 & 297 & 174 & 0.66 & 0.53 & 0.59 \\
        influential & 38 & 51 & 62 & 0.38 & 0.43 & 0.40 \\
        action & 170 & 231 & 352 & 0.33 & 0.42 & 0.37 \\
        event-relating & 1 & 28 & 0 & 1.00 & 0.03 & 0.07 \\
        RELATIONAL-TYPE &  &  &  &  &  &  \\
        attributive & 169 & 239 & 107 & 0.61 & 0.41 & 0.49 \\
        directional & 30 & 13 & 127 & 0.19 & 0.70 & 0.30 \\
        locational & 39 & 20 & 207 & 0.16 & 0.66 & 0.26 \\
        matching & 2 & 0 & 69 & 0.03 & 1.00 & 0.05 \\
        MENTAL-TYPE &  &  &  &  &  &  \\
        three-role-cognition & 45 & 51 & 34 & 0.57 & 0.47 & 0.51 \\
        two-role-cognition & 95 & 102 & 86 & 0.52 & 0.48 & 0.50 \\
        two-role-perception & 13 & 12 & 102 & 0.11 & 0.52 & 0.19 \\
        three-role-perception & 0 & 2 & 6 & 0.00 & 0.00 &  \\
        desiderative & 0 & 0 & 81 & 0.00 &  &  \\
        emotive & 0 & 0 & 87.00 & 0.00 &  & \\ \bottomrule
        \end{tabulary}
    }
    \caption{The evaluation statistics available for the PROCESS-TYPE system and few of its subsystems from the TRANSITIVITY system network}
    \label{tab:features-transitivity}
    \end{table}
    
    In Table \ref{tab:features-transitivity} \textit{mental} and \textit{relational} processes are the ones with highest $F_1$ scores: 0.64 and 0.59. They are followed by \textit{influential} and \textit{action} process types while results for the \textit{event-relating} are not conclusive because of the very small number of occurrences in the dataset. This can be read in Table \ref{tab:features-transitivity} where the number of matched segments is higher than the number of non matched corpus or parser segments. Note that considerable volume of annotations for process type sub-types are provided for mental and relational processes only \citep[153-155]{schulz2015me}.
    
    Among the \textit{mental} processes, \textit{two-role-cognition} and \textit{three-role-cognition} are parsed with highest accuracy of 51\% and 50\% correspondingly; whereas among \textit{relational} ones the \textit{attributive} process type scores the highest, 49\% while rest of them score much lower. This can be seen from the higher number of  non-matched segments for each process type for every feature.     
    
    \begin{table}[!ht]
    \centering
    \begin{tabular}{lccc}
        \toprule
        {} & {Precision} & {Recall} & {F1} \\ %\hline
        \midrule
        mean & 0.35 & 0.48 & 0.36 \\
        standard deviation & 0.32 & 0.26 & 0.19 \\
        min value & 0.00 & 0.00 & 0.05 \\
        25\% quantile & 0.07 & 0.42 & 0.24 \\
        50\% quantile & 0.33 & 0.48 & 0.39 \\
        75\% quantile & 0.59 & 0.55 & 0.51 \\
        max value & 1.00 (0.76) & 1.00 (0.70) & 0.64 \\
        \bottomrule
    \end{tabular}
    \caption{Descriptive statistics of the precision, recall and F1 scores for evaluated TRANSITIVITY features}
    \label{tab:transitivity-accuracy}
    \end{table}
    
    Looking at the entire set of evaluation results for process types, the precision and recall values vary quite a lot from a minimum of 3\% up to a maximum of 100\% and the harmonic mean, the $F_1$ score, between 7\% and 64\%  averaging to 41\%. The details can be read in Table \ref{tab:transitivity-accuracy}. The maximum of 100\% precision is a bit unfortunate because there is one instance of event-relating process found by the parser which also failed to find the other 28 thus the recall of 3\% only. So I decided to ignore this value and use the next maximum which is 76\% for mental process types. A similar case is for the matching process type which was provided only two times in the corpus but the parser generated 67 different instances of it.
    
    Next I provide an analysis of the evaluation data indicating a potential relation between the increase in the delicacy and effect it has on the parser accuracy. The accuracy of mental process detection is 64\% whereas the average accuracy for the mental sub-types (cognition, perception, desiderative and emotive) is 40\%. The same holds for relational process whose accuracy is 59\% whereas the average of its sub-types (attributive, directional, locational, matching) is only 26\%. I start by comparing the number of mental and relational segments to the sum of mental sub-type segments and sum of relational sub-type segments correspondingly. 
    
   \begin{table}[!ht]
    \noindent
    \resizebox{0.98\linewidth}{!}{%
     \begin{minipage}[t]{0.495\textwidth}
         \centering
         \resizebox{0.995\textwidth}{!}{%  
             \begin{tabulary}{1.2\textwidth}{|C|c|c|c|}
                 \hline
                 \textbf{Features} & \textbf{Manual} & \textbf{Parse} & \textbf{/} \\ \hline
                 mental & 508 & 364 & \textbf{0.72} \\ \hline
                 mental sub-types (sum of) & 320 & 549 & \textbf{1.72} \\ \hline
                 \textbf{/} & \textbf{0.63} & \textbf{1.51} &  \\ \hline
             \end{tabulary}
         }
         \caption{The ratios between \textit{mental} segments and the sum of mental sub-type segments}
         \label{tab:ratios-mental}
     \end{minipage}%
     \quad
     \begin{minipage}[t]{0.495\textwidth}
         \centering
         %    \begin{table}[!ht]
         \resizebox{0.995\textwidth}{!}{%  
             \begin{tabulary}{1.2\textwidth}{|C|c|c|c|}
                 \hline
                 \textbf{Features} & \textbf{Manual} & \textbf{Parse} & \textit{\textbf{/}} \\ \hline
                 relational & 635 & 512 & \textbf{0.8} \\ \hline
                 relational sub-types (sum of) & 512 & 750 & \textbf{1.47} \\ \hline
                 \textit{\textbf{/}} & \textbf{0.8} & \textbf{1.47} &  \\ \hline
             \end{tabulary}
         }
         \caption{The ratios between \textit{relational} segments and the sum of mental sub-type segments}
         \label{tab:ratios-relational}
         %    \end{table}
     \end{minipage}
     }
    \end{table}
    
    Table \ref{tab:ratios-mental} and \ref{tab:ratios-relational} provide each two pairs of ratios. They have very similar data content. Next I discuss the case of the mental process type only because the same applies to the case of relational processes and perhaps to other ones, but we lack data for testing this hypothesis. Thus, the first pair of ratios, provided in the lowest row, compares the number of segments with \textit{mental} feature to the sum of segments with any sub-type of \textit{mental} feature (i.e.  \textit{cognition}, \textit{perception}, \textit{emotive}, etc.). This ratio measures how well are the feature dependencies preserved across delicacy levels. The second pair of ratios, provided in the last column, compares the number of segments provided by the parser and to that available in corpus for both the mental feature and the sum of its sub-types.
    
    Table \ref{tab:ratios-mental} shows that in the corpus the number of segments with \textit{mental} feature is almost one fourth higher than what the parser provides (72 \%). This result means that probably not all the instances of a mental process have been detected by the parser (i.e. 28\% undetected). The same comparison ran on the sub-types of \textit{mental} process shows diametrically opposite results, i.e. three fourths more parser generated results than in the corpus (172\%) which is an indication of false positives. A possible explanation is the correlation between increase in delicacy and uncertainty i.e. the more delicate features are less precise in the parser results. As mentioned in the beginning of the section, uncertainty this case is manifested as a disjunction set of equally possible options. Currently there is no ranking available based on degrees of confidence. Hence the parser provides multiple feature selections from the same system (in this case MENTAL-TYPE) for the same constituent whereas there should be a single one. In the future this needs to be addressed by introducing a discrimination mechanism so that parser collects first all the possible matches and then only the most suitable is assigned possibly by using frequencies available from a corpus like OE.
    
    If we look again at Table \ref{tab:ratios-mental} and compare the number of all mental sub-type occurrences (see Figure \ref{fig:mental-types}) to the number of mental type occurrences, then we see that the ratio is quite low (63\%). As the delicacy of the features increases fewer of these features are provided in the corpus. This is a direct manifestation of difficulty in annotating with ever more delicate features. This ratio, therefore, measures the degree of incompleteness at this level of delicacy. Comparing the same ratio for the parser generated segments we notice an opposite result (151\%). This is in fact another measurement of noise generated due to uncertainty when advancing to a more delicate features as explained above. 
    
    \begin{table}[!b]
    \centering
    % \resizebox{0.8\textwidth}{!}{%
        \begin{tabulary}{0.8\textwidth}{@{}lCCCccc@{}}
        \toprule
        & Match & Corpus non-matched & Parser non-matched & Precision & Recall & F1 \\ 
        \midrule
        emoter & 91 & 70 & 57 & 0.61 & 0.57 & 0.59 \\
        phenomenon & 359 & 223 & 294 & 0.55 & 0.62 & 0.58 \\
        carrier & 267 & 263 & 244 & 0.52 & 0.50 & 0.51 \\
        cognizant & 82 & 84 & 104 & 0.44 & 0.49 & 0.47 \\
        agent & 267 & 210 & 428 & 0.38 & 0.56 & 0.46 \\
        possessed & 71 & 24 & 155 & 0.31 & 0.75 & 0.44 \\
        attribute & 162 & 241 & 170 & 0.49 & 0.40 & 0.44 \\
        affected & 93 & 70 & 663 & 0.12 & 0.57 & 0.20 \\
        \bottomrule
        \end{tabulary}
    % }
    \caption{The evaluation statistics available for the PARTICIPANT-ROLE-TYPE system from the TRANSITIVITY system network}
    \label{tab:features-participant-role-restricted}
    \end{table}
    
    So far we have discussed the process types and now let's turn attention towards the features from the PARTICIPANT-ROLE-TYPE system. Table \ref{tab:features-participant-role} presents the evaluation data along with the precision, recall and $F_1$ score for each participant role sorted according to $F_1$ score in descending order. 
    
    \begin{table}[!b]
    \centering
    \begin{tabular}{lccc}
        \toprule
        {} & {Precision} & {Recall} & {F1} \\ %\hline
        \midrule
        mean & 0.43 & 0.56 & 0.46 \\
        standard deviation & 0.16 & 0.10 & 0.12 \\
        min value & 0.12 & 0.40 & 0.20 \\
        25\% quantile & 0.37 & 0.50 & 0.44 \\
        50\% quantile & 0.46 & 0.56 & 0.46 \\
        75\% quantile & 0.53 & 0.58 & 0.53 \\
        max value & 0.61 & 0.75 & 0.59 \\
        \bottomrule
    \end{tabular}
    \caption{Descriptive statistics of the precision, recall and F1 scores for evaluated PARTICIPANT-ROLE features}
    \label{tab:features-participant-role-accuracy}
    \end{table}
    
    In this evaluation only the participant roles that appear at least 100 times in the corpus are considered. This restriction is inherited from \citet[160-162]{schulz2015me} study on OE corpus. The entire set of evaluation results for participant roles is provided in Table \ref{tab:features-participant-role} in Appendix \ref{ch:evaluation-data}. 
    
    The evaluation results for considered set of participant roles is summarised in Table \ref{tab:features-participant-role-accuracy}. The precision vary from 12\% to 61\% with an average of 43\%, and the recall values between 40\% and 75\% with an average of 56\%. The data is characterised by lower precision and higher recall. This is reflected in the $F_1$ score which is situated at a low average of almost 20\%. The maximum of 100\% recall is a bit unfortunate because there are three features with a single instance provided in the corpus and many provided in the parser output thus the precision of 0\%. So I decided to ignore this value and use the next maximum which is 75\% for possessed participant role.
    
    \vspace{1em}
    \noindent
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{evaluation-results/figures/PARTICIPANT-ROLE-distribution-PR-10.pdf}
        \captionof{figure}{The distribution of precision and recall for selected features from the PARTICIPANT-ROLE system}
        \label{fig:participant-role-precission-recall}
    \end{minipage}
    \quad
    \begin{minipage}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{evaluation-results/figures/PARTICIPANT-ROLE-distribution-F1-10.pdf}
        \captionof{figure}{The distribution of F1 score for selected features from the PARTICIPANT-ROLE system}
        \label{fig:participant-role-precission-f1}
    \end{minipage}
    \vspace{1em}
    
     The distribution of precision, recall and $F_1$ scores can be seen in Figures \ref{fig:participant-role-precission-recall} and \ref{fig:participant-role-precission-f1}. The  noticeable feature is the high peak of precision near the minimum side of the graph and fairly uniform and flat distribution of recall up to the maximum side of the graph. They translate into $F_1$ distribution as three groups: a considerable first group near the minimum pole; a second small island, formed by destination and affected participant roles, at 20\% accuracy; and a third larger group between 40\% and 60\% accuracy on the right side of the graph.
    
    In Table \ref{tab:features-participant-role} a suite of eight features (emoter, phenomenon, carrier, possessive, cognizant, agent, possessed and attribute) have $F_1$ scores descending from almost 60\% to 44\%. Destination and affected features follow with half of the previous (20\%) and then the score halves again to 11\% continuing to descend towards zero. As the accuracy decreases we can notice that multiple features display high recall spikes. This is a manifestation of the assignment of multiple roles to the same participant for the reasons explained above for the process types. 

    From the data presented in Table \ref{tab:features-participant-role} we can conclude that only the first one third of the participant roles can be considered usable with a reasonable level of confidence. It also shows a serious lack of data for the lower one third of the participant roles, which is directly reflected in low precision scores. A hypothesis that I put forward here is that in a larger corpus, with broader coverage over the participant roles, the scores for the rare features would improve considerably compared to this results. The abnormally high number of participants generated by the parser must be addressed in future work starting with an investigation of the Transitivity graph patterns generated from the PTDB, in particular for the features exhibiting recall spikes. 
    
    The compound roles such as affected-carrier, affected-possessed, agent-perceiver etc. have scored very low accuracy especially if compared to their simple counter parts. For example carrier role is detected with an accuracy of 51\% and affected role with  20\% while the compound role with very low accuracy of only 7\%. This indicates that in future work the way compound roles are assigned by the parser and how these assignments are evaluated should be rethought and a different approach taken.  
    
    The next section concludes this evaluation chapter.
    % todo finish this section

\section{Discussion}
\label{sec:evaluation-discussion}
    
    In this chapter we have discussed how the empirical evaluation of Parsimonious Vole parser has been conducted. The stage is set through a general presentation of the corpora and what the task at hand is, i.e. identifying and comparing segments available in the manually annotated corpus to those generated automatically by the parser. The comparison is performed in terms of how accurate the spans of corpus and parser segments are and how many segments with the same label are matched.
    
    In Section \ref{sec:corpus} are presented the OCD and OE corpora that were used in the current evaluation exercise. OCD corpus is used for measuring the accuracy of the constituency structure and MOOD feature assignments generated by the parser. The other corpus, OE, is used to evaluate TRANSITIVITY feature assignments. The parser output does not follow entirely the annotation methodology used in annotation of the corpus therefore there are a few differences to account for, which are explained in Section \ref{sec:differences}. 
    
    Section \ref{sec:evaluation-methodology} explains how the current evaluation is performed . It starts by defining what is evaluated i.e. labelled segments, following onto how corpus annotations and parser output are represented as batches of segments, and finally presenting how these batches are compared to one another deriving from that parser accuracy measurements and how the measurement data is structured. The alignment algorithm takes into consideration not only the exact but also the partial matches, which is discussed in part in the next section. 
    
    The next two sections, \ref{sec:syntactic-evaluation} and \ref{sec:systemic-evaluation}, present the evaluation data and discuss the findings. The evaluation of the segmentation task revealed that 71\% of the segment have identical spans and that 83\% of the segments are identical or shifted slightly (up to 5 characters). There are several ways to measure distance and among the tested ones the most significant was the geometric distance and the WindowDiff distance, while the other distances were strongly correlated to one of these two and were omitted from the discussion. 
    
    The results of evaluating constituency structure are as follows. The parser generated unit classes with an accuracy of 74\%, clause main Mood elements with an accuracy of 71.2\% and Transitivity elements with an accuracy of 81\%. 
    
    When it comes to evaluating the accuracy of systemic assignments, the measurements vary a lot not only between features at different levels of delicacy but also between sibling features within the same system. The accuracy measurements are provided for a fraction of the MOOD system network and a fraction of TRANSITIVITY system network, based on the availability of annotations in the corpus (described in Section \ref{sec:corpus}). The features from the MOOD system network are assigned, on average, with an accuracy of 59\%. The accuracy of Transitivity parsing is measured for process types and participant roles separately. The former, on average, is 36\% and the latter, on average, is 19\%. 
    
    The present evaluation results are significant in at least two major respects. First, the parser overall accuracy is not very satisfactory, although, if restricted to well performing features only, the parsing results could be considered useful in some practical situations. Second, this evaluation shows the areas that are in need of improvement and provides explanations of what could be the reason and thus setting the scene for future work improvements. 
    This is already a good starting point for investigations of certain grammar areas with considerable low performance in order identify and resolve potential problems. Also, this evaluation is the first one and constitutes the baseline for further incremental developments.
    
    Even if it is completely separate action, this evaluation can be useful for further corpus improvements as well. When I mention corpus improvement I bear in mind the OCD corpus in particular, which needs to be re-annotated by at least one more annotator and to be tested for reliability. In addition, the corpora size is fairly small and many systemic features are missing or under-represented. For example event relating, environmental and other processes are missing from the corpus just like the distinctions of action types. It would be desperately necessary and yet quite unlikely to happen, to extend the corpus annotation and include more delicate MOOD and TRANSITIVITY features to study how they vary and how accurately the parser detects them. 
    
    Next chapter will put the entire thesis into perspective and conclude the work done so far, providing new ideas and setting a tone for what needs to be done in future work to improve the current results.