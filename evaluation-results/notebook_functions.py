
from __future__ import division
import matplotlib.pyplot as plt
import pandas as pd
import ipdb # for debuging
import pandas as pd
import numpy as np
import glob
import os
import itertools
from IPython.display import HTML, Markdown, display
import re

# evaluation_repo="/home/lps/Dropbox/Publications/PhD Thesis 2015/thesis/evaluation-results/"
# evaluation_repo=os.path.dirname(os.path.abspath(__file__))
evaluation_repo=os.getcwd()+"/"

OE1_const_file_list = glob.glob(evaluation_repo+"OE1/*/*.txt_const.csv")
OE1_trans_file_list = glob.glob(evaluation_repo+"OE1/*/*.txt_trans.csv")

BTC_const_file_list = glob.glob(evaluation_repo+"BTC/*/*.txt_const.csv")
BTC_trans_file_list = glob.glob(evaluation_repo+"BTC/*/*.txt_trans.csv")

OCD_odd_const_file_list = glob.glob(evaluation_repo+"ocd/*.txt_constituency.csv")
OCD_odd_mood_file_list = glob.glob(evaluation_repo+"ocd/*.txt_mood.csv")

OCD1_const_file_list = glob.glob(evaluation_repo+"ocd1/*.txt_constituency.csv")
OCD1_mood_file_list = glob.glob(evaluation_repo+"ocd1/*.txt_mood.csv")

OCD2_const_file_list = glob.glob(evaluation_repo+"ocd2/*.txt_const.csv")
OCD2_mood_file_list = glob.glob(evaluation_repo+"ocd2/*.txt_mood.csv")

OCD_const_file_list = OCD1_const_file_list + OCD2_const_file_list
OCD_mood_file_list = OCD1_mood_file_list + OCD2_mood_file_list

# ALL_distance_analisys_file_list =  itertools.chain(
#                                     OE1_const_file_list ,
#                                     #OE1_trans_file_list ,
#                                     BTC_const_file_list , 
#                                     #BTC_trans_file_list , 
#                                     #OCD_const_file_list ,
#                                     #OCD_mood_file_list
#                                     )

def read_match_results(file_path):
    """ read an alignment file generated by the evaluation module of Parsimonious Vole"""
    columns = ["Man Id", "Interval", "Text", "Features"]
    to_delete = [u'Dist. Edit', u'Man Text', u'Auto Text', u'Man Features',u'Auto Features']
    
    book = pd.read_csv(file_path,skiprows=6)
    idx_first_empty_row = book.index[np.isnan(book.iloc[:, 0])]

    # expecting four empty rows
    assert len(idx_first_empty_row) == 4
    
    # get first the matches up to the first empty line
    matches_book =  pd.DataFrame(book[0:idx_first_empty_row[0]])
    # get the set of non-matched manual annotations
    manual_non_matched_book =  pd.DataFrame(book[ idx_first_empty_row[1]+1:idx_first_empty_row[3]])
    # get the set of non-matched parse annotations
    parse_non_matched_book =  pd.DataFrame(book[ idx_first_empty_row[3]+1:])
    
    # reshape manual_non_matched_book
    
    manual_non_matched_book.drop(to_delete,axis=1,inplace=True) 
    manual_non_matched_book.columns = columns
    manual_non_matched_book.dropna(subset=['Man Id'], how='all', inplace=True)

    # reshape parse_non_matched_book
    parse_non_matched_book.drop(to_delete,axis=1,inplace=True) 
    parse_non_matched_book.columns = columns
    parse_non_matched_book.dropna(subset=['Man Id'], how='all', inplace=True)
  
    # adding the file name
    matches_book["File"]=os.path.basename(file_path)
    manual_non_matched_book["File"]=os.path.basename(file_path)
    parse_non_matched_book["File"]=os.path.basename(file_path)
    
    # splitting the intervals
    interval_column_names_man = {0:"Man Interval Start", 1:"Man Interval End"}
    interval_column_names_auto = {0:"Auto Interval Start", 1:"Auto Interval End"}
    interval_column_names = {0:"Interval Start", 1:"Interval End"}

    intervals = matches_book["Man Interval"].str.split(pat=",",expand=True)#.rename(columns=interval_column_names_man)
    intervals[[0,1]] = intervals[[0,1]].astype(np.int64, errors='ignore')
    intervals = intervals.rename(columns=interval_column_names_man)
    matches_book = pd.concat([matches_book,intervals], axis=1)

    intervals = matches_book["Auto Interval"].str.split(pat=",",expand=True)#.rename(columns=interval_column_names_auto)
    intervals[[0,1]] = intervals[[0,1]].astype(np.int64, errors='ignore')
    intervals = intervals.rename(columns=interval_column_names_auto)
    matches_book = pd.concat([matches_book,intervals], axis=1)

    intervals = manual_non_matched_book ["Interval"].str.split(pat=",",expand=True)#.rename(columns=interval_column_names)
    intervals[[0,1]] = intervals[[0,1]].astype(np.int64, errors='ignore')
    intervals = intervals.rename(columns=interval_column_names)
    manual_non_matched_book = pd.concat([manual_non_matched_book,intervals], axis=1)
    intervals = parse_non_matched_book ["Interval"].str.split(pat=",",expand=True)#.rename(columns=interval_column_names)
    intervals[[0,1]] = intervals[[0,1]].astype(np.int64, errors='ignore')
    intervals = intervals.rename(columns=interval_column_names)
    parse_non_matched_book = pd.concat([parse_non_matched_book,intervals], axis=1)

    return matches_book, manual_non_matched_book, parse_non_matched_book

def _correct_column_types(matches, manual_nm, parse_nm):
    """ correcting known column types """
    matches["Man Id"] =  matches["Man Id"].astype(str)
    matches["Dist. Geometric"] =  matches["Dist. Geometric"].astype(float)
    matches["Dist. Edit"] =  matches["Dist. Edit"].astype(float)

    manual_nm["Man Id"] =  manual_nm["Man Id"].astype(str)
    parse_nm["Man Id"] =  parse_nm["Man Id"].astype(str)
    
    return matches, manual_nm, parse_nm

def read_batch(file_list):
    """ read a batch of alignment files generated by the evaluation module of Parsimonious Vole"""
    matches, manual_nm, parse_nm = [], [] , []
    for name in file_list:
#         print("Loading "+os.path.basename(name))
        a,b,c = read_match_results(name)
        matches.append(a)
        manual_nm.append(b)
        parse_nm.append(c)        
    return _correct_column_types(pd.concat(matches), pd.concat(manual_nm), pd.concat(parse_nm))

def aggregate_files(file_list, marker=""):
    """ aggregate the alignment files generated by the evaluation module of Parsimonious Vole"""
    # get the agregated results
    match,man,parse = read_batch(file_list)

    # now write that down into a excel file
    output = pd.ExcelWriter(evaluation_repo+"raw_agregate_"+marker+".xlsx",engine="openpyxl")

    match.to_excel(output,sheet_name="matches")
    man.to_excel(output,sheet_name="manual")
    parse.to_excel(output,sheet_name="parse")
    output.save()
    print "Done agregating."

def dsp(df):
    """ Dysplay a dataframe in IPython environment"""
    display(HTML(df.to_html()))
    
def precission(match,man,parse):
    """ true_positives / (false_positive + true_positives) """
    true_positives = match
    false_positives = parse
    try:
        return true_positives / (false_positives + true_positives)
    except:
        return -1  
    
def recall(match,man,parse):
    """true_positives / (false_negative + true_positives)"""
    try:
        true_positives = match
        false_negatives = man
        return true_positives / (false_negatives + true_positives)
    except:
        return -1
    
def f1(match,man,parse):
    """2 * precission * recall / (precission + recall)"""
    try:
        p = precission(match,man,parse)
        r = recall(match,man,parse)
        return 2 * p * r / (p + r)
    except:
        return -1
    
def calculate_prf1(matches,manual_nm,parse_nm):
    """ generate the precission, recall and f1 for each feature
    provided the matches and manual & pasre non-matches"""

    # drop the edit distance
    d=matches.drop("Dist. Edit", axis=1)

    # delete/replace verb with main 
    # d.replace("verb","main", inplace=True)
    # d.drop(d[d["Man Features"]=="verb"].index,inplace=True)

    groups = d.sort_values(["Man Features"],ascending=True).groupby("Man Features", as_index=True)

    matches_count = groups["Man Id"].count()
    matches_count.rename("Match",inplace=True)
    

    # looking into manual segments
    d = manual_nm

    # d.replace("verb","main", inplace=True)

    groups = d.sort_values(["Features"],ascending=True).groupby("Features", as_index=True)
    manual_count = groups["Man Id"].count()
    manual_count.rename("Manual",axis=0,inplace=True)


    # looking into automatic segments
    d = parse_nm
    # d.replace("verb","main", inplace=True)
    groups = d.sort_values(["Features"],ascending=True).groupby("Features", as_index=True)
    parse_count = groups["Man Id"].count()
    parse_count.rename("Parse",inplace=True)
    
    
    # merge the three series into a data frame

    stats = pd.concat([matches_count, manual_count, parse_count],axis=1, sort=False)
    
#     stats["Match"] =  pd.to_numeric(stats["Match"],downcast="integer", errors='ignore')
#     stats["Parse"] =  pd.to_numeric(stats["Parse"],downcast="integer", errors='ignore')
#     stats["Manual"] =  pd.to_numeric(stats["Manual"],downcast="integer", errors='ignore')
        
    stats["Match"] = stats["Match"].astype(np.int64, errors='ignore').fillna(0)
    stats["Parse"] = stats["Parse"].astype(np.int64, errors='ignore').fillna(0)
    stats["Manual"] = stats["Manual"].astype(np.int64, errors='ignore').fillna(0)

    stats["precission"] = stats.apply(lambda x: precission(x["Match"], x["Manual"], x["Parse"]) ,axis=1)
    stats["recall"] = stats.apply(lambda x: recall(x["Match"], x["Manual"], x["Parse"]) ,axis=1)
    stats["f1"] = stats.apply(lambda x: f1(x["Match"], x["Manual"], x["Parse"]) ,axis=1)
    
    return stats

def add_relative_values(df):
    """ add relative values for Match, Manual and Parse columns 
    
        Match% represents the percentage of matched segments in a row relative to the total sum 
        Manual% represents the percentage of unmatched manual segments in a row
        Parse% represents the percentage of unmatched parse segments in a row
    """
    df["Match%"] = df.apply(lambda x: x["Match"] / df["Match"].sum() * 100 ,axis=1)
    
    #df["Manual%"] = df.apply(lambda x: (x["Manual"] + x["Match"]) / (df["Match"].sum() + df["Manual"].sum() ) * 100 ,axis=1)
    #df["Parse%"] = df.apply(lambda x: (x["Parse"] + x["Match"]) / (df["Match"].sum() + df["Parse"].sum() ) * 100 ,axis=1)
    df["Manual%"] = df.apply(lambda x: (x["Manual"]) / (x["Match"] + x["Manual"]) * 100 ,axis=1)
    df["Parse%"] = df.apply(lambda x: (x["Parse"] ) / (x["Match"] + x["Parse"])* 100 ,axis=1)
    

    
def make_stats(matches, manual_nm, parse_nm, filters=None,drops=None):
    """ generates statistics for a group of features """
    # # ---------
    # matches, manual_nm, parse_nm = read_batch(file_list)
    # # ---------

    stats=calculate_prf1(matches,manual_nm,parse_nm)

    stats.sort_values(by="f1", inplace=True, ascending=False)
       
    #configurations = ["action","mental","relational","influential","event-relating","environmental"]
    if drops:
        stats.drop(stats.loc[stats.index.isin(drops)].index,inplace=True)
    
    if filters:
        stats = stats[stats.index.isin(filters)]
        
    add_relative_values(stats)
    
    dsp(stats)
    #     Ansolute stats
    d = stats[["Match","Manual","Parse"]]
    #d = d[stats.index.isin(filters)]
    d.plot.bar()

    plt.xlabel("Features")
    plt.ylabel("Occurences")
    #     Relative stats
    d = stats[["f1","precission","recall"]]
    d.plot.bar()

    plt.xlabel("Features")
    plt.ylabel("Score")
    
    return stats
    
#from IPython.display import Markdown, display
def printmd(string):
    display(Markdown(string))

def select_evaluation_segments(match_df, manual_nm_df, parse_nm_df,
                               only_features=[],
                               after_index=-1,
                               before_index=-1,
                               start_index=-1,
                               end_index=-1,
                               longer_than=-1,
                               shorter_than=-1,
                               from_files=[], 
                               txt_pattern="", 
                               index_compare="both", 
                               txt_compare="both", 
                               is_regex=False):
    """ from an evaluation data frames get the segments that fulfill all criteria 
        index_compare="both" ensures that the matched segments index is checked for for both manual and parsed
        index_compare="man" ensures that the matched segments index is checked only for both manual
        index_compare="auto" ensures that the matched segments index is checked only for both parsed

        same is true for txt_compare="both"|"man"|"auto"
    """
   
    # filtering the matched ones
    matches_filtered = match_df
    if only_features:
        matches_filtered = matches_filtered[matches_filtered["Man Features"].isin(only_features) ]
    if start_index > -1:
        if index_compare == "man":
            matches_filtered = matches_filtered[ matches_filtered["Man Interval Start"] == start_index ]
        else:    
            matches_filtered = matches_filtered[ matches_filtered["Auto Interval Start"] == start_index ]
    if after_index > -1:
        if index_compare == "man":
            matches_filtered = matches_filtered[ matches_filtered["Man Interval Start"] >= after_index ]
        elif index_compare == "both":
            matches_filtered = matches_filtered[ matches_filtered["Man Interval Start"] >= after_index ]
            matches_filtered = matches_filtered[ matches_filtered["Auto Interval Start"] >= after_index ]
        else:
            matches_filtered = matches_filtered[ matches_filtered["Auto Interval Start"] >= after_index ]
    if end_index > -1:
        if index_compare == "man":
            matches_filtered = matches_filtered[ matches_filtered["Man Interval End"] == end_index ]
        else:
            matches_filtered = matches_filtered[ matches_filtered["Auto Interval End"] == end_index ]
    if before_index > -1:
        if index_compare == "man":
            matches_filtered = matches_filtered[ matches_filtered["Man Interval End"] <= before_index ]
        elif index_compare == "both":
            matches_filtered = matches_filtered[ matches_filtered["Man Interval End"] <= before_index ]
            matches_filtered = matches_filtered[ matches_filtered["Auto Interval End"] <= before_index ]
        else:
            matches_filtered = matches_filtered[ matches_filtered["Auto Interval End"] <= before_index ]
    if from_files:
        matches_filtered = matches_filtered[ matches_filtered["File"].isin(from_files)]
    if txt_pattern:
        if txt_compare == "man":
            matches_filtered = matches_filtered[ matches_filtered["Man Text"].str.contains(pat=txt_pattern,regex=is_regex,na=False)]
        elif txt_compare == "both":            
            matches_filtered = matches_filtered[ matches_filtered["Man Text"].str.contains(pat=txt_pattern,regex=is_regex,na=False)]
            matches_filtered = matches_filtered[ matches_filtered["Auto Text"].str.contains(pat=txt_pattern,regex=is_regex,na=False)]
        else:
            matches_filtered = matches_filtered[ matches_filtered["Auto Text"].str.contains(pat=txt_pattern,regex=is_regex,na=False)]
    if longer_than > -1:
        matches_filtered["Length"] = matches_filtered["Man Interval End"] - matches_filtered["Man Interval Start"]
        matches_filtered = matches_filtered[ matches_filtered["Length"] >= longer_than].drop(columns=["Length"])
    if shorter_than > -1:
        matches_filtered["Length"] = matches_filtered["Man Interval End"] - matches_filtered["Man Interval Start"]
        matches_filtered = matches_filtered[ matches_filtered["Length"] <= shorter_than].drop(columns=["Length"])


    # filtering the manual non matched
    nm_filtered = manual_nm_df
    if only_features:
        nm_filtered = nm_filtered[nm_filtered["Features"].isin(only_features) ]
    if after_index > -1:
        nm_filtered = nm_filtered[ nm_filtered["Interval Start"] >= after_index ]
    if before_index > -1:    
        nm_filtered = nm_filtered[ nm_filtered["Interval End"] <= before_index ]
    if start_index > -1:
        nm_filtered = nm_filtered[ nm_filtered["Interval Start"] == start_index ]
    if end_index > -1:
        nm_filtered = nm_filtered[ nm_filtered["Interval End"] == end_index ]
    if from_files:    
        nm_filtered = nm_filtered[nm_filtered["File"].isin(from_files) ]
    if txt_pattern:
        nm_filtered = nm_filtered[nm_filtered["Text"].str.contains(pat=txt_pattern,regex=is_regex,na=False) ]
    if longer_than > -1:
        nm_filtered["Length"] = nm_filtered["Interval End"] - nm_filtered["Interval Start"]
        nm_filtered = nm_filtered[ nm_filtered["Length"] >= longer_than].drop(columns=["Length"])
    if shorter_than > -1:
        nm_filtered["Length"] = nm_filtered["Interval End"] - nm_filtered["Interval Start"]
        nm_filtered = nm_filtered[ nm_filtered["Length"] <= shorter_than].drop(columns=["Length"])

    manual_nm_filtered = nm_filtered
    
    # filtering the parsed non matched
    nm_filtered = parse_nm_df
    if only_features:
        nm_filtered = nm_filtered[nm_filtered["Features"].isin(only_features) ]
    if after_index > -1:
        nm_filtered = nm_filtered[ nm_filtered["Interval Start"] >= after_index ]
    if before_index > -1:    
        nm_filtered = nm_filtered[ nm_filtered["Interval End"] <= before_index ]        
    if start_index > -1:
        nm_filtered = nm_filtered[ nm_filtered["Interval Start"] == start_index ]
    if end_index > -1:
        nm_filtered = nm_filtered[ nm_filtered["Interval End"] == end_index ]
    if from_files:    
        nm_filtered = nm_filtered[nm_filtered["File"].isin(from_files) ]
    if txt_pattern:
        nm_filtered = nm_filtered[nm_filtered["Text"].str.contains(pat=txt_pattern,regex=is_regex,na=False) ]
    if longer_than > -1:
        nm_filtered["Length"] = nm_filtered["Interval End"] - nm_filtered["Interval Start"]
        nm_filtered = nm_filtered[ nm_filtered["Length"] >= longer_than].drop(columns=["Length"])
    if shorter_than > -1:
        nm_filtered["Length"] = nm_filtered["Interval End"] - nm_filtered["Interval Start"]
        nm_filtered = nm_filtered[ nm_filtered["Length"] <= shorter_than].drop(columns=["Length"])
    parse_nm_filtered = nm_filtered
    
    return matches_filtered, manual_nm_filtered, parse_nm_filtered

def dsp_evaluation_segments(match_df, manual_nm_df, parse_nm_df,
                            only_features=[],
                            after_index=-1,
                            before_index=-1,
                            start_index=-1,
                            end_index=-1,
                            longer_than=-1,
                            shorter_than=-1,
                            from_files=[], 
                            txt_pattern="", 
                            index_compare="both", 
                            txt_compare="both", 
                            is_regex=False):

    """ Dysplay the evaluation selected segments """
    mm,m,p = select_evaluation_segments(match_df=match_df, manual_nm_df=manual_nm_df, parse_nm_df=parse_nm_df,
                                    only_features=only_features,
                                    after_index=after_index,
                                    before_index=before_index,
                                    start_index=start_index,
                                    end_index=end_index,
                                    longer_than=longer_than,
                                    shorter_than=shorter_than,
                                    from_files=from_files,
                                    txt_pattern=txt_pattern,
                                    index_compare=index_compare,
                                    txt_compare=txt_compare,
                                    is_regex=is_regex)

    printmd("### Filtered segments ["+str(len(mm))+"/"+str(len(m))+"/"+str(len(p))+"]")
    printmd("* features = "+ (str(only_features) if only_features else "All") +
    "\n* after index = " + ( str(after_index) if after_index>-1 else "Any" ) +
    "\n* before index = " + ( str(before_index) if before_index>-1 else "Any" ) +
    "\n* start index = " + ( str(start_index) if start_index>-1 else "None" ) +
    "\n* end index = " + ( str(end_index) if end_index>-1 else "None" ) +
    "\n* from files = " + ( str(from_files) if from_files else "All" ) )
    printmd("### Matched segments ["+str(len(mm))+"]")
    dsp(mm)
    printmd("### Manual non matched segments ["+str(len(m))+"]")    
    dsp(m)
    printmd("### Parsed non matched segments ["+str(len(p))+"]")
    dsp(p)

    return mm,m,p

def find_near_same_segments(matches, manual_nm, parse_nm):
    """ for each segment see if there are suplicates for it """
    
    matches_reduced, manual_nm_reduced, parse_nm_reduced = matches, manual_nm, parse_nm
    # going over the matches
    for index, row in matches.iterrows():
        # search the shortest text
        # txt = row["Man Text"] if len(row["Man Text"]) < len(row["Auto Text"]) else row["Auto Text"]
        mm, m, p = select_evaluation_segments(matches, manual_nm, parse_nm,
                                   only_features=[ row["Auto Features"] ],
                                   start_index=row["Auto Interval Start"],
                                   end_index=row["Auto Interval End"],
                                   from_files=[row["File"]],
                                   txt_pattern="",#row["Auto Text"],
                                   index_compare="auto", 
                                   is_regex=False)
        # find unselected macthded segments 
        if len(mm)==0:
            printmd("**ERROR:** did not select the existent row")
            printmd( str([ row["Man Features"] ])+str(row["Auto Interval Start"])+str(row["Auto Interval End"])+str([row["File"]], row["Auto Text"]) )
        
        # find duplicate matches
        if len(mm)>1 :
            printmd("**WARNING:** selected multiple matched segments")
            dsp(mm)

        # find duplicate nom matched parses
        if len(p)>0 :
            printmd("**WARNING:** almost identical unmatched **parser** segment, probably parser redudancy")
            dsp(mm)
            dsp(p)

        # find duplicate non matched manual segments
        if len(m)>0 :
            printmd("**WARNING:** almost identical unmatched **manual** segment, probably annotation redundancy")
            dsp(mm)
            dsp(m)
    
    return matches_reduced, manual_nm_reduced, parse_nm_reduced
