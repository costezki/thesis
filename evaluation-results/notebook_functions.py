
from __future__ import division
import matplotlib.pyplot as plt
# import pandas as pd
import ipdb # for debuging
import pandas as pd
import numpy as np
import glob
import os
import itertools
from IPython.display import HTML, Markdown, display
import re
from nltk.metrics import segmentation as segmentation
from nltk.metrics import distance as distance
import math 
import scipy.spatial.distance as sd

# evaluation_repo="/home/lps/Dropbox/Publications/PhD Thesis 2015/thesis/evaluation-results/"
# evaluation_repo=os.path.dirname(os.path.abspath(__file__))
evaluation_repo=os.getcwd()+"/"

OE1_const_file_list = glob.glob(evaluation_repo+"OE1/*/*.txt_const.csv")
OE1_trans_file_list = glob.glob(evaluation_repo+"OE1/*/*.txt_trans.csv")

BTC_const_file_list = glob.glob(evaluation_repo+"BTC/*/*.txt_const.csv")
BTC_trans_file_list = glob.glob(evaluation_repo+"BTC/*/*.txt_trans.csv")

OCD_odd_const_file_list = glob.glob(evaluation_repo+"ocd/*.txt_constituency.csv")
OCD_odd_mood_file_list = glob.glob(evaluation_repo+"ocd/*.txt_mood.csv")

OCD1_const_file_list = glob.glob(evaluation_repo+"ocd1/*.txt_constituency.csv")
OCD1_mood_file_list = glob.glob(evaluation_repo+"ocd1/*.txt_mood.csv")

OCD2_const_file_list = glob.glob(evaluation_repo+"ocd2/*.txt_const.csv")
OCD2_mood_file_list = glob.glob(evaluation_repo+"ocd2/*.txt_mood.csv")

OCD_const_file_list = OCD1_const_file_list + OCD2_const_file_list
OCD_mood_file_list = OCD1_mood_file_list + OCD2_mood_file_list

FIGURE_PATH = evaluation_repo + "figures/"


# ALL_distance_analisys_file_list =  itertools.chain(
#                                     OE1_const_file_list ,
#                                     #OE1_trans_file_list ,
#                                     BTC_const_file_list , 
#                                     #BTC_trans_file_list , 
#                                     #OCD_const_file_list ,
#                                     #OCD_mood_file_list
#                                     )

def read_match_results(file_path):
    """ read an alignment file generated by the evaluation module of Parsimonious Vole"""
    columns = ["Man Id", "Interval", "Text", "Features"]
    to_delete = [u'Dist. Edit', u'Man Text', u'Auto Text', u'Man Features',u'Auto Features']
    
    book = pd.read_csv(file_path,skiprows=6)
    idx_first_empty_row = book.index[np.isnan(book.iloc[:, 0])]

    # expecting four empty rows
    assert len(idx_first_empty_row) == 4
    
    # get first the matches up to the first empty line
    matches_book =  pd.DataFrame(book[0:idx_first_empty_row[0]])
    # get the set of non-matched manual annotations
    manual_non_matched_book =  pd.DataFrame(book[ idx_first_empty_row[1]+1:idx_first_empty_row[3]])
    # get the set of non-matched parse annotations
    parse_non_matched_book =  pd.DataFrame(book[ idx_first_empty_row[3]+1:])
    
    # reshape manual_non_matched_book
    
    manual_non_matched_book.drop(to_delete,axis=1,inplace=True) 
    manual_non_matched_book.columns = columns
    manual_non_matched_book.dropna(subset=['Man Id'], how='all', inplace=True)

    # reshape parse_non_matched_book
    parse_non_matched_book.drop(to_delete,axis=1,inplace=True) 
    parse_non_matched_book.columns = columns
    parse_non_matched_book.dropna(subset=['Man Id'], how='all', inplace=True)
  
    # adding the file name
    matches_book["File"]=os.path.basename(file_path)
    manual_non_matched_book["File"]=os.path.basename(file_path)
    parse_non_matched_book["File"]=os.path.basename(file_path)
    
    # splitting the intervals
    interval_column_names_man = {0:"Man Interval Start", 1:"Man Interval End"}
    interval_column_names_auto = {0:"Auto Interval Start", 1:"Auto Interval End"}
    interval_column_names = {0:"Interval Start", 1:"Interval End"}

    intervals = matches_book["Man Interval"].str.split(pat=",",expand=True)#.rename(columns=interval_column_names_man)
    intervals[[0,1]] = intervals[[0,1]].astype(np.int64, errors='ignore')
    intervals = intervals.rename(columns=interval_column_names_man)
    matches_book = pd.concat([matches_book,intervals], axis=1)

    intervals = matches_book["Auto Interval"].str.split(pat=",",expand=True)#.rename(columns=interval_column_names_auto)
    intervals[[0,1]] = intervals[[0,1]].astype(np.int64, errors='ignore')
    intervals = intervals.rename(columns=interval_column_names_auto)
    matches_book = pd.concat([matches_book,intervals], axis=1)

    intervals = manual_non_matched_book ["Interval"].str.split(pat=",",expand=True)#.rename(columns=interval_column_names)
    intervals[[0,1]] = intervals[[0,1]].astype(np.int64, errors='ignore')
    intervals = intervals.rename(columns=interval_column_names)
    manual_non_matched_book = pd.concat([manual_non_matched_book,intervals], axis=1)
    intervals = parse_non_matched_book ["Interval"].str.split(pat=",",expand=True)#.rename(columns=interval_column_names)
    intervals[[0,1]] = intervals[[0,1]].astype(np.int64, errors='ignore')
    intervals = intervals.rename(columns=interval_column_names)
    parse_non_matched_book = pd.concat([parse_non_matched_book,intervals], axis=1)

    return matches_book, manual_non_matched_book, parse_non_matched_book

def _correct_column_types(matches, manual_nm, parse_nm):
    """ correcting known column types """
    matches["Man Id"] =  matches["Man Id"].astype(str)
    matches["Dist. Geometric"] =  matches["Dist. Geometric"].astype(float)
    matches["Dist. Edit"] =  matches["Dist. Edit"].astype(float)

    manual_nm["Man Id"] =  manual_nm["Man Id"].astype(str)
    parse_nm["Man Id"] =  parse_nm["Man Id"].astype(str)
    
    return matches, manual_nm, parse_nm

def read_batch(file_list):
    """ read a batch of alignment files generated by the evaluation module of Parsimonious Vole"""
    matches, manual_nm, parse_nm = [], [] , []
    for name in file_list:
#         print("Loading "+os.path.basename(name))
        a,b,c = read_match_results(name)
        matches.append(a)
        manual_nm.append(b)
        parse_nm.append(c)        
    return _correct_column_types(pd.concat(matches), pd.concat(manual_nm), pd.concat(parse_nm))

def aggregate_files(file_list, marker=""):
    """ aggregate the alignment files generated by the evaluation module of Parsimonious Vole"""
    # get the agregated results
    match,man,parse = read_batch(file_list)

    # now write that down into a excel file
    output = pd.ExcelWriter(evaluation_repo+"raw_agregate_"+marker+".xlsx",engine="openpyxl")

    match.to_excel(output,sheet_name="matches")
    man.to_excel(output,sheet_name="manual")
    parse.to_excel(output,sheet_name="parse")
    output.save()
    print "Done agregating."

def dsp(df):
    """ Dysplay a dataframe in IPython environment"""
    display(HTML(df.to_html()))

def false_omissions(match,man,parse):
    """ false_negatives / (false_positive + true_positives) """
    true_positives = match
    false_positives = parse
    false_negatives = man

    if (false_positives + true_positives)==0: return np.nan
    return false_negatives / (false_positives + true_positives)


def miss_rate(match,man,parse):
    """ false_negatives / (false_positive + true_positives) """
    true_positives = match
    false_positives = parse
    false_negatives = man

    if (false_positives + false_negatives)==0: return np.nan
    return false_negatives / (false_positives + false_negatives)
    
def precission(match,man,parse):
    """ true_positives / (false_positive + true_positives) """
    true_positives = match
    false_positives = parse
    if (false_positives + true_positives)==0: return np.nan
    return true_positives / (false_positives + true_positives)
    
    
def recall(match,man,parse):
    """true_positives / (false_negative + true_positives)"""
    true_positives = match
    false_negatives = man
    if (false_negatives + true_positives)==0: return np.nan
    return true_positives / (false_negatives + true_positives)
    
def f1(match,man,parse):
    """2 * precission * recall / (precission + recall)"""
    p = precission(match,man,parse)
    r = recall(match,man,parse)
    if (p + r)==0: return np.nan
    return 2 * p * r / (p + r)
    
def calculate_prf1(matches,manual_nm,parse_nm):
    """ generate the precission, recall and f1 for each feature
    provided the matches and manual & pasre non-matches"""

    # drop the edit distance
    d=matches.drop("Dist. Edit", axis=1)

    # delete/replace verb with main 
    # d.replace("verb","main", inplace=True)
    # d.drop(d[d["Man Features"]=="verb"].index,inplace=True)

    groups = d.sort_values(["Man Features"],ascending=True).groupby("Man Features", as_index=True)

    matches_count = groups["Man Id"].count()
    matches_count.rename("Match",inplace=True)
    

    # looking into manual segments
    d = manual_nm

    # d.replace("verb","main", inplace=True)

    groups = d.sort_values(["Features"],ascending=True).groupby("Features", as_index=True)
    manual_count = groups["Man Id"].count()
    manual_count.rename("Manual",axis=0,inplace=True)


    # looking into automatic segments
    d = parse_nm
    # d.replace("verb","main", inplace=True)
    groups = d.sort_values(["Features"],ascending=True).groupby("Features", as_index=True)
    parse_count = groups["Man Id"].count()
    parse_count.rename("Parse",inplace=True)
    
    
    # merge the three series into a data frame

    stats = pd.concat([matches_count, manual_count, parse_count],axis=1, sort=False)
    
#     stats["Match"] =  pd.to_numeric(stats["Match"],downcast="integer", errors='ignore')
#     stats["Parse"] =  pd.to_numeric(stats["Parse"],downcast="integer", errors='ignore')
#     stats["Manual"] =  pd.to_numeric(stats["Manual"],downcast="integer", errors='ignore')
        
    stats["Match"] = stats["Match"].astype(np.int64, errors='ignore').fillna(0)
    stats["Parse"] = stats["Parse"].astype(np.int64, errors='ignore').fillna(0)
    stats["Manual"] = stats["Manual"].astype(np.int64, errors='ignore').fillna(0)

    stats["precission"] = stats.apply(lambda x: precission(x["Match"], x["Manual"], x["Parse"]) ,axis=1)
    stats["recall"] = stats.apply(lambda x: recall(x["Match"], x["Manual"], x["Parse"]) ,axis=1)
    stats["f1"] = stats.apply(lambda x: f1(x["Match"], x["Manual"], x["Parse"]) ,axis=1)
    
    return stats

def add_relative_values(df):
    """ add relative values for Match, Manual and Parse columns 
    
        Match% represents the percentage of matched segments in a row relative to the total sum 
        Manual% represents the percentage of unmatched manual segments in a row
        Parse% represents the percentage of unmatched parse segments in a row
    """
    df["Match%"] = df.apply(lambda x: x["Match"] / df["Match"].sum() * 100 ,axis=1)
    
    #df["Manual%"] = df.apply(lambda x: (x["Manual"] + x["Match"]) / (df["Match"].sum() + df["Manual"].sum() ) * 100 ,axis=1)
    #df["Parse%"] = df.apply(lambda x: (x["Parse"] + x["Match"]) / (df["Match"].sum() + df["Parse"].sum() ) * 100 ,axis=1)
    df["Manual%"] = df.apply(lambda x: (x["Manual"]) / (x["Match"] + x["Manual"]) * 100 ,axis=1)
    df["Parse%"] = df.apply(lambda x: (x["Parse"] ) / (x["Match"] + x["Parse"])* 100 ,axis=1)
    

    
def make_stats(matches, manual_nm, parse_nm, filters=None,drops=None):
    """ generates statistics for a group of features """
    # # ---------
    # matches, manual_nm, parse_nm = read_batch(file_list)
    # # ---------

    stats=calculate_prf1(matches,manual_nm,parse_nm)

    stats.sort_values(by="f1", inplace=True, ascending=False)
       
    #configurations = ["action","mental","relational","influential","event-relating","environmental"]
    if drops:
        stats.drop(stats.loc[stats.index.isin(drops)].index,inplace=True)
    
    if filters:
        stats = stats[stats.index.isin(filters)]
        
    add_relative_values(stats)
    
    dsp(stats)
    #     Ansolute stats
    d = stats[["Match","Manual","Parse"]]
    #d = d[stats.index.isin(filters)]
    d.plot.bar()

    plt.xlabel("Features")
    plt.ylabel("Occurences")
    #     Relative stats
    d = stats[["f1","precission","recall"]]
    d.plot.bar()

    plt.xlabel("Features")
    plt.ylabel("Score")
    
    return stats
    
#from IPython.display import Markdown, display
def printmd(string):
    display(Markdown(string))

def select_evaluation_segments(match_df, manual_nm_df, parse_nm_df,
                               only_features=[],
                               after_index=-1,
                               before_index=-1,
                               start_index=-1,
                               end_index=-1,
                               longer_than=-1,
                               shorter_than=-1,
                               from_files=[], 
                               txt_pattern="", 
                               index_compare="both", 
                               txt_compare="both", 
                               is_regex=False):
    """ from an evaluation data frames get the segments that fulfill all criteria 
        index_compare="both" ensures that the matched segments index is checked for for both manual and parsed
        index_compare="man" ensures that the matched segments index is checked only for both manual
        index_compare="auto" ensures that the matched segments index is checked only for both parsed

        same is true for txt_compare="both"|"man"|"auto"
    """
   
    # filtering the matched ones
    matches_filtered = match_df
    if only_features:
        matches_filtered = matches_filtered[matches_filtered["Man Features"].isin(only_features) ]
    if start_index > -1:
        if index_compare == "man":
            matches_filtered = matches_filtered[ matches_filtered["Man Interval Start"] == start_index ]
        else:    
            matches_filtered = matches_filtered[ matches_filtered["Auto Interval Start"] == start_index ]
    if after_index > -1:
        if index_compare == "man":
            matches_filtered = matches_filtered[ matches_filtered["Man Interval Start"] >= after_index ]
        elif index_compare == "both":
            matches_filtered = matches_filtered[ matches_filtered["Man Interval Start"] >= after_index ]
            matches_filtered = matches_filtered[ matches_filtered["Auto Interval Start"] >= after_index ]
        else:
            matches_filtered = matches_filtered[ matches_filtered["Auto Interval Start"] >= after_index ]
    if end_index > -1:
        if index_compare == "man":
            matches_filtered = matches_filtered[ matches_filtered["Man Interval End"] == end_index ]
        else:
            matches_filtered = matches_filtered[ matches_filtered["Auto Interval End"] == end_index ]
    if before_index > -1:
        if index_compare == "man":
            matches_filtered = matches_filtered[ matches_filtered["Man Interval End"] <= before_index ]
        elif index_compare == "both":
            matches_filtered = matches_filtered[ matches_filtered["Man Interval End"] <= before_index ]
            matches_filtered = matches_filtered[ matches_filtered["Auto Interval End"] <= before_index ]
        else:
            matches_filtered = matches_filtered[ matches_filtered["Auto Interval End"] <= before_index ]
    if from_files:
        matches_filtered = matches_filtered[ matches_filtered["File"].isin(from_files)]
    if txt_pattern:
        if txt_compare == "man":
            matches_filtered = matches_filtered[ matches_filtered["Man Text"].str.contains(pat=txt_pattern,regex=is_regex,na=False)]
        elif txt_compare == "both":            
            matches_filtered = matches_filtered[ matches_filtered["Man Text"].str.contains(pat=txt_pattern,regex=is_regex,na=False)]
            matches_filtered = matches_filtered[ matches_filtered["Auto Text"].str.contains(pat=txt_pattern,regex=is_regex,na=False)]
        else:
            matches_filtered = matches_filtered[ matches_filtered["Auto Text"].str.contains(pat=txt_pattern,regex=is_regex,na=False)]
    if longer_than > -1:
        matches_filtered["Length"] = matches_filtered["Man Interval End"] - matches_filtered["Man Interval Start"]
        matches_filtered = matches_filtered[ matches_filtered["Length"] >= longer_than].drop(columns=["Length"])
    if shorter_than > -1:
        matches_filtered["Length"] = matches_filtered["Man Interval End"] - matches_filtered["Man Interval Start"]
        matches_filtered = matches_filtered[ matches_filtered["Length"] <= shorter_than].drop(columns=["Length"])


    # filtering the manual non matched
    nm_filtered = manual_nm_df
    if only_features:
        nm_filtered = nm_filtered[nm_filtered["Features"].isin(only_features) ]
    if after_index > -1:
        nm_filtered = nm_filtered[ nm_filtered["Interval Start"] >= after_index ]
    if before_index > -1:    
        nm_filtered = nm_filtered[ nm_filtered["Interval End"] <= before_index ]
    if start_index > -1:
        nm_filtered = nm_filtered[ nm_filtered["Interval Start"] == start_index ]
    if end_index > -1:
        nm_filtered = nm_filtered[ nm_filtered["Interval End"] == end_index ]
    if from_files:    
        nm_filtered = nm_filtered[nm_filtered["File"].isin(from_files) ]
    if txt_pattern:
        nm_filtered = nm_filtered[nm_filtered["Text"].str.contains(pat=txt_pattern,regex=is_regex,na=False) ]
    if longer_than > -1:
        nm_filtered["Length"] = nm_filtered["Interval End"] - nm_filtered["Interval Start"]
        nm_filtered = nm_filtered[ nm_filtered["Length"] >= longer_than].drop(columns=["Length"])
    if shorter_than > -1:
        nm_filtered["Length"] = nm_filtered["Interval End"] - nm_filtered["Interval Start"]
        nm_filtered = nm_filtered[ nm_filtered["Length"] <= shorter_than].drop(columns=["Length"])

    manual_nm_filtered = nm_filtered
    
    # filtering the parsed non matched
    nm_filtered = parse_nm_df
    if only_features:
        nm_filtered = nm_filtered[nm_filtered["Features"].isin(only_features) ]
    if after_index > -1:
        nm_filtered = nm_filtered[ nm_filtered["Interval Start"] >= after_index ]
    if before_index > -1:    
        nm_filtered = nm_filtered[ nm_filtered["Interval End"] <= before_index ]        
    if start_index > -1:
        nm_filtered = nm_filtered[ nm_filtered["Interval Start"] == start_index ]
    if end_index > -1:
        nm_filtered = nm_filtered[ nm_filtered["Interval End"] == end_index ]
    if from_files:    
        nm_filtered = nm_filtered[nm_filtered["File"].isin(from_files) ]
    if txt_pattern:
        nm_filtered = nm_filtered[nm_filtered["Text"].str.contains(pat=txt_pattern,regex=is_regex,na=False) ]
    if longer_than > -1:
        nm_filtered["Length"] = nm_filtered["Interval End"] - nm_filtered["Interval Start"]
        nm_filtered = nm_filtered[ nm_filtered["Length"] >= longer_than].drop(columns=["Length"])
    if shorter_than > -1:
        nm_filtered["Length"] = nm_filtered["Interval End"] - nm_filtered["Interval Start"]
        nm_filtered = nm_filtered[ nm_filtered["Length"] <= shorter_than].drop(columns=["Length"])
    parse_nm_filtered = nm_filtered
    
    return matches_filtered, manual_nm_filtered, parse_nm_filtered

def dsp_evaluation_segments(match_df, manual_nm_df, parse_nm_df,
                            only_features=[],
                            after_index=-1,
                            before_index=-1,
                            start_index=-1,
                            end_index=-1,
                            longer_than=-1,
                            shorter_than=-1,
                            from_files=[], 
                            txt_pattern="", 
                            index_compare="both", 
                            txt_compare="both", 
                            is_regex=False):

    """ Dysplay the evaluation selected segments """
    mm,m,p = select_evaluation_segments(match_df=match_df, manual_nm_df=manual_nm_df, parse_nm_df=parse_nm_df,
                                    only_features=only_features,
                                    after_index=after_index,
                                    before_index=before_index,
                                    start_index=start_index,
                                    end_index=end_index,
                                    longer_than=longer_than,
                                    shorter_than=shorter_than,
                                    from_files=from_files,
                                    txt_pattern=txt_pattern,
                                    index_compare=index_compare,
                                    txt_compare=txt_compare,
                                    is_regex=is_regex)

    printmd("### Filtered segments ["+str(len(mm))+"/"+str(len(m))+"/"+str(len(p))+"]")
    printmd("* features = "+ (str(only_features) if only_features else "All") +
    "\n* after index = " + ( str(after_index) if after_index>-1 else "Any" ) +
    "\n* before index = " + ( str(before_index) if before_index>-1 else "Any" ) +
    "\n* start index = " + ( str(start_index) if start_index>-1 else "None" ) +
    "\n* end index = " + ( str(end_index) if end_index>-1 else "None" ) +
    "\n* from files = " + ( str(from_files) if from_files else "All" ) )
    printmd("### Matched segments ["+str(len(mm))+"]")
    dsp(mm)
    printmd("### Manual non matched segments ["+str(len(m))+"]")    
    dsp(m)
    printmd("### Parsed non matched segments ["+str(len(p))+"]")
    dsp(p)

    return mm,m,p

def find_near_same_segments(matches, manual_nm, parse_nm):
    """ for each segment see if there are suplicates for it """
    
    matches_reduced, manual_nm_reduced, parse_nm_reduced = matches, manual_nm, parse_nm
    # going over the matches
    for index, row in matches.iterrows():
        # search the shortest text
        # txt = row["Man Text"] if len(row["Man Text"]) < len(row["Auto Text"]) else row["Auto Text"]
        mm, m, p = select_evaluation_segments(matches, manual_nm, parse_nm,
                                   only_features=[ row["Auto Features"] ],
                                   start_index=row["Auto Interval Start"],
                                   end_index=row["Auto Interval End"],
                                   from_files=[row["File"]],
                                   txt_pattern="",#row["Auto Text"],
                                   index_compare="auto", 
                                   is_regex=False)
        # find unselected macthded segments 
        if len(mm)==0:
            printmd("**ERROR:** did not select the existent row")
            printmd( str([ row["Man Features"] ])+str(row["Auto Interval Start"])+str(row["Auto Interval End"])+str([row["File"]], row["Auto Text"]) )
        
        # find duplicate matches
        if len(mm)>1 :
            printmd("**WARNING:** selected multiple matched segments")
            dsp(mm)

        # find duplicate nom matched parses
        if len(p)>0 :
            printmd("**WARNING:** almost identical unmatched **parser** segment, probably parser redudancy")
            dsp(mm)
            dsp(p)

        # find duplicate non matched manual segments
        if len(m)>0 :
            printmd("**WARNING:** almost identical unmatched **manual** segment, probably annotation redundancy")
            dsp(mm)
            dsp(m)
    
    return matches_reduced, manual_nm_reduced, parse_nm_reduced


####################################################
# functions added in evaluation 2.0
####################################################

EXACT_MATCH_COLUMN = "Matched (exactly only)"
CLOSE_MATCH_COLUMN = "Matched (closely only)"
MANUAL_COLUMN = "Corpus non-matched"
PARSE_COLUMN = "Parser non-matched"
COMBINED_MATCH_COLUMN = "Matched"


def aggregate_data_by_feature(matches, manual_nm, parse_nm):
    """ 
        For a batch read from the evalaution files return the 
        significant matched/non-matached countings per feature.
    """
    # looking into segment matche. Compile counting. Matches are split into two: zero distance and non-zero distance
    exact_matches = matches.loc[matches['Dist. Geometric'] == 0]
    close_matches = matches.loc[matches['Dist. Geometric'] != 0]

    exact_match_groups = exact_matches.sort_values(["Man Features"],ascending=True).groupby("Man Features", as_index=True)
    exact_match_count = exact_match_groups["Man Id"].count()
    exact_match_count.rename(EXACT_MATCH_COLUMN,inplace=True)

    close_match_groups = close_matches.sort_values(["Man Features"],ascending=True).groupby("Man Features", as_index=True)
    close_match_count = close_match_groups["Man Id"].count()
    close_match_count.rename(CLOSE_MATCH_COLUMN,inplace=True)

    combined_match_groups = matches.sort_values(["Man Features"],ascending=True).groupby("Man Features", as_index=True)
    combined_match_count = combined_match_groups["Man Id"].count()
    combined_match_count.rename(COMBINED_MATCH_COLUMN,inplace=True)

    
    # looking into manual segments

    manual_nm_groups = manual_nm.sort_values(["Features"],ascending=True).groupby("Features", as_index=True)
    manual_count = manual_nm_groups["Man Id"].count()
    manual_count.rename(MANUAL_COLUMN,axis=0,inplace=True)

    # looking into automatic segments

    parse_nm_groups = parse_nm.sort_values(["Features"],ascending=True).groupby("Features", as_index=True)
    parse_count = parse_nm_groups["Man Id"].count()
    parse_count.rename(PARSE_COLUMN,axis=0,inplace=True)

    stats = pd.concat([exact_match_count, close_match_count, combined_match_count, manual_count, parse_count],axis=1, sort=False)
    return stats.fillna(0)

PRECISSION_COLUMN = "Precission"
RECALL_COLUMN = "Recall"
F1_COLUMN = "F1"
MISS_RATE_COLUMN = "Miss rate"
FALSE_OMISSIONs_COLUMN = "False omission rate"


def __accuracy_statistics(aggregate_data_by_feature, match_column=COMBINED_MATCH_COLUMN):
    """ Accuracy statistics for the dataset"""
    result = pd.DataFrame()
    MC = match_column
    
    result[PRECISSION_COLUMN] = aggregate_data_by_feature.apply(lambda x: precission(x[MC], x[MANUAL_COLUMN], x[PARSE_COLUMN]) ,axis=1)
    result[RECALL_COLUMN] = aggregate_data_by_feature.apply(lambda x: recall(x[MC], x[MANUAL_COLUMN], x[PARSE_COLUMN]) ,axis=1)
    result[F1_COLUMN] = aggregate_data_by_feature.apply(lambda x: f1(x[MC], x[MANUAL_COLUMN], x[PARSE_COLUMN]) ,axis=1)
    result[MISS_RATE_COLUMN] = aggregate_data_by_feature.apply(lambda x: miss_rate(x[MC], x[MANUAL_COLUMN], x[PARSE_COLUMN]) ,axis=1)
    result[FALSE_OMISSIONs_COLUMN] = aggregate_data_by_feature.apply(lambda x: false_omissions(x[MC], x[MANUAL_COLUMN], x[PARSE_COLUMN]) ,axis=1)
    
    return result

def accuracy_statistics_exact(aggregate_data_by_feature):
    """  """
    return __accuracy_statistics(aggregate_data_by_feature, match_column=EXACT_MATCH_COLUMN )  

def accuracy_statistics_close(aggregate_data_by_feature):
    """  """
    return __accuracy_statistics(aggregate_data_by_feature, match_column=CLOSE_MATCH_COLUMNN )  

def accuracy_statistics_combined(aggregate_data_by_feature):
    """  """
    return __accuracy_statistics(aggregate_data_by_feature, match_column=COMBINED_MATCH_COLUMN )  


# 
# setting the parameters for all plots
# 
HATCH_DENSITY = 2
HATCH_PATTERNS = [ 
                  "+"*HATCH_DENSITY , 
                  "x"*HATCH_DENSITY, 
                  "."*HATCH_DENSITY, 
                  "O"*HATCH_DENSITY, 
                  "o"*HATCH_DENSITY, 
                  "*"*HATCH_DENSITY, 
                  "/"*HATCH_DENSITY , 
                  "\\"*HATCH_DENSITY , 
                  "|" *HATCH_DENSITY, 
                  "-"*HATCH_DENSITY , 
                    ]                 


cmap = plt.get_cmap("tab20c")
COLORS = cmap(np.arange(3)*4)

def bar_plot(df, xlabel="", ylabel="", filename = None):
    """
        Draw a bar plot from a dataframe using hashes and colours
    """
    f = plt.figure()
    ax = f.add_subplot(111)
    df.plot(ax=ax, kind='bar', legend=False)

    bars = ax.patches
    hatches = list(itertools.chain( *[[h] * len(df) for h in HATCH_PATTERNS] ))
    
    for bar, hatch in zip(bars, hatches):
        bar.set_hatch(hatch)

    ax.legend(loc='center right', bbox_to_anchor=(1, 1), ncol=2)
    
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)

    if filename:
        f.savefig(FIGURE_PATH+filename+".pdf", bbox_inches='tight')

    # a bit of cleanup
    plt.show()
    plt.clf()


def ltx(df, filename="test", caption="My caption here", to_display=False):
    """ Dysplay a dataframe as latex tabular"""
    table = '\\begin{table}[!ht]\n\\centering\n'
    table += df.to_latex(na_rep=0, float_format='{:,.2f}'.format, bold_rows=False, )
    table +='\\caption{'+caption+'}\n'
    table +='\\label{tab:'+filename+'}\n'
    table +='\\end{table}'

    if to_display:
        dsp(df)

    if filename:
        with open(FIGURE_PATH+filename+".tex",'w') as tf:
            tf.write(table)
            

PLOT_STATS_XLABEL = "Feature"
PLOT_STATS_YLABEL_SCORE = "Score"
PLOT_STATS_YLABEL_OCCURENCES = "Occurences"
PLOT_STATS_F1_columns = [PRECISSION_COLUMN,RECALL_COLUMN,F1_COLUMN]
PLOT_STATS_Err_columns = [MISS_RATE_COLUMN, FALSE_OMISSIONs_COLUMN]
PLOT_STATS_DATA_columns_exact_also = [EXACT_MATCH_COLUMN,COMBINED_MATCH_COLUMN,MANUAL_COLUMN, PARSE_COLUMN]
PLOT_STATS_DATA_columns = [COMBINED_MATCH_COLUMN,MANUAL_COLUMN, PARSE_COLUMN]


def make_stats2(aggregate_data_by_feature, eval_name, filters=[], make_exact_also=False):
    """
    """
    stats = aggregate_data_by_feature.copy()
    if filters:
        stats = stats[stats.index.isin(filters)]
    
    stats_exact_match    = accuracy_statistics_exact(stats)
    stats_combined_match = accuracy_statistics_combined(stats)
      
    stats.sort_values(by=COMBINED_MATCH_COLUMN, inplace=True, ascending=False)
    stats_exact_match.sort_values(by=F1_COLUMN, inplace=True, ascending=False)
    stats_combined_match.sort_values(by=F1_COLUMN, inplace=True, ascending=False)

    # original data
    if make_exact_also:
        bar_plot(stats[PLOT_STATS_DATA_columns_exact_also],PLOT_STATS_XLABEL, PLOT_STATS_YLABEL_OCCURENCES, eval_name+"-data")
        ltx(stats[PLOT_STATS_DATA_columns_exact_also],eval_name+"-data")
    else:
        bar_plot(stats[PLOT_STATS_DATA_columns],PLOT_STATS_XLABEL, PLOT_STATS_YLABEL_OCCURENCES, eval_name+"-data")
        ltx(stats[PLOT_STATS_DATA_columns],eval_name+"-data")

    # precission
    bar_plot(stats_combined_match[PLOT_STATS_F1_columns],PLOT_STATS_XLABEL, PLOT_STATS_YLABEL_SCORE, eval_name+"-combined-"+"F1")
    ltx(stats_combined_match[PLOT_STATS_F1_columns],eval_name+"-combined-"+"F1")
    
    if make_exact_also:
        bar_plot(stats_exact_match[PLOT_STATS_F1_columns],PLOT_STATS_XLABEL, PLOT_STATS_YLABEL_SCORE, eval_name+"-exact-"+"F1")
        ltx(stats_exact_match[PLOT_STATS_F1_columns],eval_name+"-exact-"+"F1")
        
    # errors
    bar_plot(stats_combined_match[PLOT_STATS_Err_columns],PLOT_STATS_XLABEL, PLOT_STATS_YLABEL_SCORE, eval_name+"-combined-"+"errors")
    ltx(stats_combined_match[PLOT_STATS_Err_columns],eval_name+"-combined-"+"errors")

    if make_exact_also:
        bar_plot(stats_exact_match[PLOT_STATS_Err_columns],PLOT_STATS_XLABEL, PLOT_STATS_YLABEL_SCORE, eval_name+"-exact-"+"errors")
        ltx(stats_exact_match[PLOT_STATS_Err_columns],eval_name+"-exact-"+"errors")
    

PREFIX_RELATIVE = "(%) "
SUFFIX_RELATIVE = ""
    
def __relative_statistics(aggregate_data_by_feature, mc = COMBINED_MATCH_COLUMN):
    """
    """
    result = pd.DataFrame()
    
    result[str(PREFIX_RELATIVE+mc+SUFFIX_RELATIVE)] = aggregate_data_by_feature.apply(lambda x: x[mc] / len(aggregate_data_by_feature) * 100 ,axis=1)
    result[str(PREFIX_RELATIVE+MANUAL_COLUMN+SUFFIX_RELATIVE)] = aggregate_data_by_feature.apply(lambda x: (x[MANUAL_COLUMN]) / (x[mc] + x[MANUAL_COLUMN]) * 100  ,axis=1)
    result[str(PREFIX_RELATIVE+PARSE_COLUMN+SUFFIX_RELATIVE)] = aggregate_data_by_feature.apply(lambda x: (x[PARSE_COLUMN]) / (x[mc] + x[PARSE_COLUMN]) * 100  ,axis=1)
    
    return result

def relative_statistics_exact(aggregate_data_by_feature):
    """    """
    return __relative_statistics(aggregate_data_by_feature, mc = EXACT_MATCH_COLUMN)

def relative_statistics_combined(aggregate_data_by_feature):
    """    """
    return __relative_statistics(aggregate_data_by_feature, mc = COMBINED_MATCH_COLUMN)

def rename_features(matches, manual_nm, parse_nm, feature_replacement):
   
    mat, man, par = matches.copy(), manual_nm.copy(), parse_nm.copy()
    
    mat["Man Features"].replace(feature_replacement, inplace=True)
    man["Features"].replace(feature_replacement, inplace=True)
    par["Features"].replace(feature_replacement, inplace=True)
    
    return mat, man, par

def drop_features(matches, manual_nm, parse_nm, drops):
   
    mat, man, par = matches.copy(), manual_nm.copy(), parse_nm.copy()
    
    mat = mat[~mat["Man Features"].isin(drops)]
    man = man[~man["Features"].isin(drops)]
    par = par[~par["Features"].isin(drops)]
    
    return mat, man, par

# 
# plaing with distances here 
# 
def to_segmentation_metric_form(s1, s2, return_window_size=False):
    """
        turn the two segments into sequences of zero and one, 
        where zero represent segment content while one represents a break
    """
    
    # half the average size of the segments
    buffer_window = int(((s1[1]-s1[0])+(s2[1]-s2[0]))/4)
    
    def __generate_mock_segment(start, end , offset, buffer_window):
        return '0'*(buffer_window + start - offset) + '1' + '0'* (end-offset) + '1' + '0'*(buffer_window)
    
    offset = min(s1+s2)
    ss1 = __generate_mock_segment(start=s1[0], end=s1[1], offset=offset, buffer_window=buffer_window)
    ss2 = __generate_mock_segment(start=s2[0], end=s2[1], offset=offset, buffer_window=buffer_window)
    
    if len(ss1)>len(ss2): ss2+='0'*(len(ss1)-len(ss2))
    if len(ss1)<len(ss2): ss1+='0'*(len(ss2)-len(ss1))
    
    if return_window_size:
        return ss1, ss2, buffer_window
    
    return ss1, ss2

DISTANCE_GEOMETRIC = "Geometric"
DISTANCE_EDIT = "Levinstein"
DISTANCE_HAMMING = "Generalised Hamming"
DISTANCE_Windowdiff = "WindowDiff"
DISTANCE_PK = "Pk"

MAN_START = 'Man Interval Start'
MAN_END = 'Man Interval End'
AUTO_START = 'Auto Interval Start'
AUTO_END = 'Auto Interval End'
MAN_TEXT = 'Man Text'
AUTO_TEXT = "Auto Text"

DISTANCES = [DISTANCE_EDIT, DISTANCE_GEOMETRIC, DISTANCE_HAMMING, DISTANCE_PK, DISTANCE_Windowdiff]

def make_distances(matches):
    """
        returns the matches with additional distance columns
    """
    df = matches.copy()
        
    df[DISTANCE_GEOMETRIC] = df.apply(lambda x: sd.euclidean( [x[MAN_START],x[MAN_END]], [x[AUTO_START],x[AUTO_END]] ), axis=1 )
    df[DISTANCE_EDIT] = df.apply(lambda x: distance.edit_distance( x[MAN_TEXT], x[AUTO_TEXT] ), axis=1 )
    df[DISTANCE_HAMMING] = df.apply(lambda x: segmentation.ghd( *to_segmentation_metric_form([x[MAN_START],x[MAN_END]], [x[AUTO_START],x[AUTO_END]] ) ), axis=1 )
    # windowfill k will be 1/2 average segment length
    df[DISTANCE_Windowdiff] = df.apply(lambda x: segmentation.windowdiff(
        *to_segmentation_metric_form([x[MAN_START],x[MAN_END]], [x[AUTO_START],x[AUTO_END]], return_window_size=True))  , axis=1 )
    df[DISTANCE_PK] = df.apply(lambda x: segmentation.pk( *to_segmentation_metric_form([x[MAN_START],x[MAN_END]], [x[AUTO_START],x[AUTO_END]] ) ), axis=1 )
    return df

def filter_batch(matches, manual_nm, parse_nm):
    """
        Filter the manual and parse non matched sets to contain 
        only features that have been also matched.
    """
    filter = matches["Man Features"].unique()
    manual_nm_filtered = manual_nm.loc[manual_nm["Features"].isin(filter) ] 
    parse_nm_filtered = parse_nm.loc[parse_nm["Features"].isin(filter) ] 
    return matches, manual_nm_filtered, parse_nm_filtered