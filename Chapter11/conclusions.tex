\chapter{Conclusions}
\label{ch:conclusions}

    This thesis aims at a reliable modular method for parsing English text with Systemic Functional Grammars. To achieve this goal I designed a pipeline, which, starting from a dependency parse of a sentence, generates a SFL-like constituency structure serving as a syntactic backbone, and then enriches that structure with various systemic features.

    In this process, the first milestone is the creation of the constituency structure. Chapter \ref{ch:sfg} describes the essential theoretical foundations of two branches of SFL: the Sydney and Cardiff schools. It also provides a critical analysis in order to reconcile the diverging points on the rank scale, unit classes, the constituency structure, treatment of coordination, grammatical unit structure and clause boundaries; and states what is the adopted position on each point. 

    In order to create the constituency structure from the dependency structure there needs to be a mechanism in place to provide a mapping between the two both at the theoretical and grammatical levels. The theoretical account of the dependency grammar and how it is related to SFL was described in Chapter \ref{ch:dependency-grammar}. The practical aspects of the process, such as the algorithms and the enactment of inter-grammatical mapping rules, were described in Chapter \ref{ch:parsing-algorithm}. 

    Before describing the parsing pipeline, to make clear what the basic ingredients of this implementation are and how the algorithms are coded, Chapter \ref{ch:data-structures} introduced the basic data structures and operations. These structures were defined from a computer science point of view emulating the needed SFL concepts. The main structures are attribute-value dictionaries, ordered lists with logical operators and a few graph types. %, their structure and how they are used in the following chapters which detail the parsing process. 
    In addition, the basic operations relevant for the parsing pipeline such as conditional traversal and querying of nodes and edges, graph matching, pattern-based node selection, insertion and update were also described.
    
    % These range from a few graph types, simple attribute-value dictionaries and ordered lists with logical operators. In addition to this, a set of specific graph operations have been defined to perform pattern matching and system network traversals.

    Once the constituency structure is created, the second milestone is to enrich it with systemic features. Because systemic features can be associated with or derived from the (dependency and constituency) graph fragments, in this work, graph pattern matching is a cornerstone operation used for adding features to constituent units and inserting new or missing constituents. These operations were described in detail in the second part of Chapter \ref{ch:data-structures}, and Chapter \ref{ch:enrichment-stage} outlined how these operations are used for the enrichment of the constituency backbone with systemic features.

    The quality of graph patterns impacts directly on the outcome of the parser. The more precise the graph patterns are the smaller the false omission and miss rate in the parser output is, and thus the number of errors in general decreases while the accuracy of feature enrichment increases. This was shown in the evaluation result discussion in Chapter \ref{ch:evaluation} in general, and in Section \ref{sec:systemic-evaluation} in particular. 
    
    It is often the case for the TRANSITIVITY network that the graph patterns, in their canonical form, list the mandatory participants of a semantic configuration. In practice, however, instances of such configurations may leave unrealised up to two mandatory participants. And so, if applied in their canonical form the patterns will not identify such instances. In this thesis, mock constituents (null elements) are created in the places where the presumed constituents should be, allowing in this way matches with canonical graph patterns. 
    
    To identify and create the covert participants I turned to Government and Binding theory, which accounts for this. In doing so, this thesis brings two more contributions: (a) the theoretical mapping from GBT into dependency structures covered in \mbox{Chapter \ref{ch:gbt}} and (b) a concrete implementation of how to perform creation of the null elements described in Chapter \ref{sec:placing-null-elements}.

    In Chapter \ref{ch:evaluation} was described the empirical evaluation. The aim of the assessment, in general, was to determine how accurately the text analysis is generated; and, in particular, how well the parser performs at unit boundary detection (i.e text segmentation), unit class assignment, element assignment and feature selections. 
    
    The data show that the parser assigns classes to the constituent units with an accuracy of 74\% and clause main Mood elements were detected with an accuracy of 71.2\%, while the Transitivity elements were detected with an accuracy of 81\%. 
    
    When it comes to evaluating the accuracy of systemic assignments, the measured accuracy varies drastically across delicacy levels and between the sibling features within the same system. This has been addressed for the MOOD and TRANSITIVITY systems in Section \ref{sec:systemic-evaluation}. The features from the MOOD system network were assigned, on average, with an accuracy of 59\%. The accuracy of the TRANSITIVITY system network was measured separately for the PROCESS-TYPE system and the PARTICIPANT-ROLE-TYPE system. The accuracy of the former, on average, is 36\% and of the latter, on average, is 46\%. 

    Next I present how the main research questions were addressed and what the main contributions of this thesis have been.
    

\section{Research questions and main findings}
    % todo write
    In Chapter \ref{ch:introduction} six research questions were asked. This section shows how those research questions are addressed in the current thesis and what the theoretical and practical outcomes are.
    
    One of the main theoretical contributions of this thesis is the investigation to what degree cross-theoretical bridges can be established between SFL and other theories of grammar, formulated as Research question \ref{question:reuse-broad}. 
    The approach to answering this broad question was to further ask more specific questions. In particular I have focused on studying correspondences to Dependency Grammar and Government and Binding Theory, formulated in Research questions \ref{question:reuse-dg} and \ref{question:reuse-gbt}. 

    % RQ2. To what degree the syntactic structure of GD and SFG are compatible to undergo a transformation from one into another. 
    First, Research question \ref{question:reuse-dg} on the degree to which the syntactic structure of the Dependency and that of Systemic Functional Grammar are compatible to undergo a transformation from one into another was fully addressed in this thesis showing that they are compatible and that the goal of transforming from one into the other is feasible. The support for this claim is provided in Section \ref{sec:cross-theoretical-bridge}, which addresses in detail the cross-theoretical links between the Dependency and the Systemic Functional theory of grammar. This cross-theoretical bridge constitutes a fundamental principle for further deriving transformation rules from a dependency representation into a systemic functional one. Such rules are enacted, in the parsing pipeline, to create the systemic constituency structure as laid out in Section \ref{sec:creation-constituency-graph}. 
    
    % RQ3. How can GBT be used for detecting places of null elements in the context of SFL constituency structure. 
    Second, Research question \ref{question:reuse-gbt} about the usability of GBT for detecting places of null elements in the context of SFL constituency structure was explored in depth with positive results. This is addressed in Section \ref{sec:placing-null-elements}, where rules, principles and generalisations from GB theory are translated into DG and SFG frameworks. These translations serve directly the goal of identifying places where (and by which relations) the null elements should be injected. The translated rules are realised in the form of graph patterns explained in detail in Section \ref{sec:creation-empty-elements}.

    % RQ1. To what extent resources and techniques from other areas of computational linguistics can be reused for the purpose of SFL parsing and how?
    Addressing Research questions \ref{question:reuse-dg} and \ref{question:reuse-gbt}
    and establishing cross-theoretical bridges to DG and GBT constitute answers for Research question \ref{question:reuse-broad}. The conducted theoretical investigation accompanied by the practical implementation and the evaluation results show that aiming at reuse, in SFL contexts, of positive results from other areas of computational linguistics constitutes not only a desirable but also a feasible goal. This, however, does not guarantee in practice maximal accuracy and the extent to which the goal is achieved depends to a large extent on the implementation. 
    
    As another contribution, this thesis offers an investigation on how suitable graph patterns are in detecting systemic features and enriching the constituency structure. In the current approach to parsing, graph patterns play a similar role as the realisation rules play in the process of natural language generation with SFG. They serve as a language for systematising grammatical realisations, and constitute a convenient form of representing grammatical features employing both structural and lexico-structural patterns. Graph patterns and the matching methods developed in this thesis can potentially be applied for expressing many other grammatical features than the ones presented here such as the remaining more delicate MOOD and TRANSITIVITY features, those covered in the THEME or and other system networks.

    % RQ5. To what degree systemic delicacy can be reached using syntactic patterns alone without any lexical semantic resources. 
    Research question \ref{question:pattern-delicacy} on the extent to which graph patterns can be used to represent systemic features based solely on structural aspects was addressed in Section \ref{sec:mood-patterns} (focused MOOD system network). It is shown that some of the least delicate systems can be dealt with only by structural patterns, however, as delicacy increases, the inclusion of lexis into the graph patterns is inevitable. Moreover, for TRANSITIVITY features, escaping the lexis is not possible at all, and constitutes the main reason for employing a lexical-semantic resource such as PTDB. This only confirms the already known strong link between grammar and lexis, which SFL considers a unitary lexico-grammar (defined in Section \ref{sec:wording}).

    % RQ6. How suitable is PTDB as a resource for Transitivity parsing? 
    In the end, Research question \ref{question:ptdb-suitable} on whether the PTDB is suitable as a lexical-semantic resource for Transitivity parsing was addressed in Chapters \ref{ch:enrichment-stage} and \ref{ch:evaluation}. I explained how graph patterns can be generated automatically from PTDB before describing how it was turned into a machine readable resource. Nonetheless, the evaluation of the currently implemented method to assign Transitivity features does not provide encouraging results, reaching only 42\% accuracy (10\% less that for Mood features). These performance indicators are explained in Section \ref{sec:systemic-evaluation-TRANSITIVITY}, and further discussed in Section \ref{sec:evaluation-discussion}. This level of accuracy, among others, is due to the currently implemented enrichment mechanism, which applies all the matching graph patterns to the constituency structure instead of applying the one with highest probability. This means that higher accuracy can be achieved provided that the implemented approach is improved by reducing the number of patterns per feature. The degree to which the accuracy will improve if the enrichment mechanism is enhanced remains a question for future work. 
    
    Finally, the evaluation results presented in Chapter\ref{ch:evaluation} are significant in at least two major respects. First, the parser accuracy of generating SFL constituency structures is comparable to or slightly lower than the accuracy achieved by previous attempts, e.g. 76\% by \citet{Souter1996} and 81\% by \citet{ODonoghue91}. However, the parser generates feature rich output which sets apart Parsimonious Vole from other parsers. The features in the generated output could be already considered useful in some practical situations where identifying in text Mood or Transitivity features is needed.
    Second, this study shows which areas are in need of improvement and provides hints on what can be improved. Also, this evaluation can be considered an initial baseline for incremental development in future work.
    
\section{Limitations and future work}
\label{sec:future-work}

    This work has a number of limitations. This section introduces the most important ones along with improvements that are desirable or worth considering.
    
\subsubsection{Parsimonious Vole parser grammar}
    
    The grammar proposed in Chapter \ref{ch:the-grammar} is a combination of elements taken from both the Cardiff and the Sydney grammars. %These grammar parts have been selected for parsing text by detecting linguistic features as patterns (in text and structure) and gradually increasing the level of semantics in the detected features.
    Even if the chosen grammar parts have been carefully motivated, explained and argued for as a whole, how well they fit together still requires scrutiny by grammarians and a validation with larger corpora. 

    %The graph patterns have been manually created, which is error prone and requires careful validation. %It is also difficult to confirm graph patterns grammatical conformance from the empirical study presented here alone. %Also, as mentioned above, the grammar employed in this work deviates from a standard grammar attempting to combine aspects of the Sydney and Cardiff grammars, which falls in the domain of grammar development.
    
    The graph patterns were manually created, which is error prone and requires careful validation. This work can be supported and facilitated by a workbench editor, debugger and validator for graph patterns and systemic features combined. The workbench could build on top of and extend the UAM Corpus tool with these new functionalities. As no editor for grammatical graph patterns exists yet, developing one in the future is desirable.

\subsubsection{Graph patterns from Nigel grammar}

    One important experience this thesis provides is the use of graph patterns for detecting systemic features, based on structural and lexical cues in the provided constituency structures. For the parser implementation, all of the MOOD patterns were created manually while the TRANSITIVITY patterns were created from a simple lexical-semantic database, the Process Type Database \citep{Neale2002}. 
    
    At the same time, the Nigel grammar \citep{Matthiessen95-all}, the largest existing SFL grammar \citep[27]{Bateman2008}, was not employed in this work even if it is very relevant. In part this is due to the reasons explained in Chapter \ref{ch:sota}, i.e. the previous attempts to parse with full SFL grammars directly had to accept certain limitations. In future work, however, investigating how graph patterns can be generated from the system network realisation rules available in Nigel will be very valuable and highly desirable work. Not only can it save time and reduce potential errors of the manual authoring of graph patterns, but it can provide a very rich set of graph patterns covering system networks outside the scope of this work.

\subsubsection{Adoption of verbal group}

    The current grammar does not include the verbal group unit but treats the elements of what would be a verbal unit as elements of the clause. This decision is motivated in Section \ref{sec:verbal-grpoup-and-clause-division} and is in line with the Nigel grammar and with the proposal put forward by the Cardiff grammar. This resolves the problem of discontinuity in the syntactic units which was an issue for the current implementation.
    
    \begin{exe}
        \ex\label{ex:feel-cold} Are you feeling cold?
    \end{exe}
    
    A simple example of a discontinuity is provided in Example \ref{ex:feel-cold}. The verbal group here is formed of the Auxiliary ``are'' and the Main verb ``feeling''. In principle, in the syntactic analysis, the units of analysis should be continuous. This is known to not always be the case as illustrated by Example \ref{ex:feel-cold} where the subject ``you'' splits the verbal group in two.
    
    Adopting a gap resilient constituency structure would permit inclusion into the generated analysis not only of verbal groups but also enable Thematic analysis, which often employs discontinuous units, and the adoption of other unit classes.

\subsubsection{Transition to semantically motivated unit classes}

    Cardiff unit classes are semantically motivated when compared to the more syntactic ones in the Sydney grammar. This is stated in \citet[193--194]{Fawcett2000} and was briefly presented in Section \ref{sec:cardiff-theory-grammar} and further discussed in Section \ref{sec:discussion-unit-classes}.

    For example, the nominal structure proposed in the Cardiff grammar (discussed in Section \ref{sec:nominal-group}), uses elements that are more semantic in nature than the syntactic and functionally motivated ones offered in the Sydney grammar. For example compare various types of determiners: representational, quantifying, typic and partitive, in the Cardiff grammar and only the deictic determiner in the Sydney grammar where the distinctions by types are provided in systemic features rather than distinct unit classes. 
    
    In order to shift towards semantically motivated nominal unit structure two problems need to be addressed: (a) how to detect semantic heads and (b) how to craft (if none exists) a lexical-semantic resource to support detection of various determiners in the nominal group. Building lexical-semantic resources asked at point (b) represents a potential solution for point (a) as well. Employing some of the existing resources such as the Nigel grammar, because it is built in Sydney style, could and most likely be a suitable starting point for addressing point (b). In addition, other non-SFL lexical resources such as WordNet \citep{Miller1995} or FrameNet \citep{Baker1998} could be considered in this context. Yet resorting to these lexical resources would not be a straightforward solution and would require more adaptations so that they are useful in the SFL domain.   

    The same holds for Adverbial and Adjectival groups (Section \ref{sec:advectival-adverbial-groups}), which in the Cardiff grammar are split into Quality and Quantity groups. Existent lexical resources such as WordNet \citep{Miller1995} or FrameNet\citep{Baker1998} combined with the delicate classification proposed by \citet{Tucker1997} may yield positive results in parsing with Cardiff unit classes. 
    Just as in the case of verb groups discussed in the previous sections, moving towards semantically motivated unit classes would greatly benefit applications requiring deeper natural language understanding. However, this will likely come at the cost of making the parsing much harder and thus a trade-off might be needed. 

\subsubsection{More delicate TRANSITIVITY graph patterns}

    The PTDB \citep{Neale2002} is the first and only lexical-semantic resource for the Cardiff Transitivity metafunction. In its original form, this resource was not machine readable, with its usability limited to dictionary-like search by linguists in the process of manual text analysis. It was rich in human understandable comments and remarks across all fields and so not fully formal enough to be employed in computational tasks. In the scope of the current work the PTDB has been cleaned and brought into a machine readable form. %This is far from exhausting its potential as a lexical-grammatical resource for semantic parsing. 

    In mainstream computational linguistics, there are several lexical-semantic resources used for Semantic Role Labelling (a task similar to Transitivity parsing), such as FrameNet \citep{Baker1998} and VerbNet \citep{Kipper2008}. Mapping or combining PTDB with these resources into a new one would yield benefits for both: potentially inspiring the internal organisation for VerbNet and extending the coverage of PTDB.

    Combining PTDB with VerbNet for example, would be my first choice in the task of improving Transitivity analysis for the following reasons. PTDB is well semantically systematised according to the Cardiff Transitivity system, however, it lacks any links to syntactic manifestations. VerbNet, on the other hand, contains an excellent mapping to the syntactic patterns in which each verb occurs, each with associated semantic representations of participant roles and some first order logic representation. %However, the systematisation of frames and participant roles could benefit from a more robust basis of categorisation. 
    Also, the lexical coverage of VerbNet is twice as wide as than that of PTDB. 

    Resorting to resources like FrameNet or WordNet could bring other benefits. For example, FrameNet has a set of annotated examples for every frame which, after transformation into the Transitivity system, could be used as a training corpus for machine learning algorithms. 
    
    % todo in which sense explain, what how
    % Another potential benefit would be generating semantic constraints (for example in terms of WordNet \citep{Miller1995} synsets or GUM \citep{Bateman1995,Bateman2010} classes) for every participant role in the system.

    % PTDB can benefit from mappings with GUM ontology which formalises the experiential model of Sydney school. First by increasing delicacy (at at the moment it covers only three top levels of the system) and second by importing constraints on process types and participant roles from the Nigel grammar \citep{Matthiessen1985}. To achieve this, one would have to first map the Cardiff and the Sydney Transitivity systems and second extract lexical entries from the Nigel grammar along with adjacent systemic selections. 
\subsubsection{Towards speech function analysis}

    As Robin Fawcett explains \citep{Fawcett2011}, Halliday's approach to Mood analysis differs from that of Transitivity in the way that the former is not ``pushed forward towards semantics'' as the latter is. This claim, however, is controversial and not endorsed by the Sydney grammarians. %who stand by the meaningfulness of the MOOD system network.
    The meaning proposed by Fawcett in the Cardiff MOOD system network is similar to and incorporates concepts from Speech Act Theory \citep{Austin1975} or its later advancements \citep{Searle1969}. Such theories, in mainstream linguistics, are placed under the umbrella of pragmatics (which Sydney grammarians reject). Operating with concepts such as speech acts, called in SFL \textit{speech functions} \citep{Hasan84-ways}, would take the interpersonal text analysis to a new level of meaning with potential benefits in applications where interactivity is a feature of primary concern.
    
    % Having a semantically systematised MOOD system would take the interpersonal text analysis into a realm compatible with Speech Act Theory proposed by \citet{Austin1975} or its latter advancements such the \citet{Searle1969} which, in mainstream linguistics, are placed under the umbrella of pragmatics. 

    Halliday proposes a simple system of speech functions \citep[136]{Halliday2013} (considered as part of semantics and outside grammar) which Fawcett develops into a quite delicate system network \citep{Fawcett2011}. It is worth exploring ways to implement Fawcett's latest developments especially that the two are not conflicting but complementing each other. In future work it can be explored how to use the Hallidayan MOOD system as a foundation to transit towards the Cardiff MOOD system (a merger of semantic and grammatical systems). Such exploration can be facilitated by the fact that Sydney MOOD system network has already been implemented and described in the current work.

\subsubsection{Adoption of group complexing}

    The group complexing structures are well described in the Sydney grammar \citep[567--592]{Halliday2013}. Such structures are not considered in the current work except for the particular case of conjunction treatment, which is described in Section \ref{sec:coordination}. Adopting a general framework of unit complexing is highly beneficial as it contributes to a more meaningful analysis. The immediate applications of group complexing, in the context of this thesis, can be seen in the case of verbal group complexes presented next. 

    % \subsubsection{Towards semantic verbal groups and complexes}
    The \textit{one main verb per clause} principle of the Cardiff school that I adopted in this thesis (briefly discussed in Section \ref{sec:verbal-grpoup-and-clause-division}) provides a basis for simple and reliable syntactic structures. Also, it represents a simple clause boundary detection rule. The alternative is adopting the concept of verbal group, simple and complex, as proposed by the Sydney school in \citet[396--418, 567--592]{Halliday2013}, a much richer and complex approach. The verb complex provides a richer semantically motivated description \citep[567--592]{Halliday2013}, however, analysing text with such constructs is difficult and subject to ambiguities.

    \begin{table}[!ht]
    	\centering
    	\begin{tabular}{|c|c|c|c|}
    		\hline
    		{\it Ants} & {\it keep}                                    & {\it biting}                                   & {\it me}    \\ \hline
    		Subject    & Finite                                        & Predicator                                     & complement  \\ \hline
    		Actor      & \multicolumn{2}{c|}{Process: Material}                                                          & Goal/Medium \\ \hline
    		& \multicolumn{2}{c|}{\begin{tabular}[c]{@{}c@{}}Verbal group complex \\ {expansion, elaborative, time-phase, durative} \\ $\alpha \longrightarrow =\beta$ \end{tabular}} &             \\ \hline
    	\end{tabular}
    	\caption{Sydney sample analysis of a clause with a \textit{verbal group complex}}
    	\label{tab:example-syndey-vb}
    \end{table}

    \begin{table}[!ht]
    	\centering
    	\begin{tabular}{|c|c|c|c|c|}
    		\hline
    		{\it Ants} & {\it keep}          & -             & {\it biting}    & {\it me}   \\ \hline
    		Subject    & Finite/Main Verb           & \multicolumn{3}{c|}{Complement}              \\ \hline
    		Agent      & Process: Influential & \multicolumn{3}{c|}{Phenomena}               \\ \hline
    		\multicolumn{2}{|c|}{}           & Subject (null) & Main Verb       & Complement \\ \hline
    		\multicolumn{2}{|c|}{}           & Agent         & Process: Action & Affected   \\ \hline
    	\end{tabular}
    	\caption{Cardiff sample analysis of a clause \textit{embedded} into another}
    	\label{tab:example-cardiff-vb}
    \end{table}

    One way to approach this is in two steps (similarly to semantic head detection discussed in Section \ref{sec:heads}): first, generating the syntactic analysis and then enriching it to a more meaningful analysis. Even though an approach in two steps such as the one suggested here is subject to criticism, in part, it can already be implemented by considering Cardiff influential process types (implemented as part of Transitivity parsing).
    
    Consider the sample analyses in Tables \ref{tab:example-syndey-vb} and \ref{tab:example-cardiff-vb}. The two-clause analysis proposed by the Cardiff school can be quite intuitively transformed into a single experiential structure with the top clause expressing a set of aspectual features of the process in the lower (embedded) clause just like the Sydney analysis in Table \ref{tab:example-syndey-vb}. 

    The class of \textit{influential} processes proposed in the Cardiff transitivity system was introduced to handle expressions of process aspects through other lexical verbs. I consider it as a class of pseudo-processes with a set of well defined and useful syntactic functions but with incomplete semantic descriptions. The analysis with influential process could be used as an intermediary step towards a more meaningful analysis, such as the one suggested by the Sydney grammar. Alternatively, the analysis process could be redesigned to generate complex verbal units directly taking into account the available lexical-syntactic resources. This rule of thumb is described in Generalisation \ref{def:merging-influential}.

    \begin{generalization}[Merging influential clauses] \label{def:merging-influential}
    	When the top clause has an influential process and the lower (embedded) one has any of the other processes, then the two clauses can be merged into one and the two verbs into a verb complex enriched with aspectual features.
    \end{generalization}
    
    Of course, this raises a set of problems that are worth investigating. First, the connections and mappings between the influential process system network described in the Cardiff grammar and the system of verbal group complex described in the Sydney grammar \citep[589]{Halliday2013} should be investigated. Second, one should investigate how this merger impacts the syntactic structure. 

    The benefits of such a merger lead to an increased comprehensiveness, not only of the Transitivity analysis, illustrated by the examples in Tables \ref{tab:example-syndey-vb} and \ref{tab:example-cardiff-vb}, but potentially apply to the modal assessment illustrated by Examples \ref{ex:modalityass1} and \ref{ex:modalityass2} and similar phenomena. 
    
    \begin{exe}
    	\ex\label{ex:modalityass1} \textit{I think} I've been pushed forward; \textit{I don't really know}, \citep[183]{Halliday2013}
    	\ex\label{ex:modalityass2} \textit{I believe} Sheridan once said you would've made an excellent pope. \citep[182]{Halliday2013}
    \end{exe}

\subsubsection{Taxis analysis}
    
    In the Sydney grammar, the logico-semantic relations employed to describe inter-clausal relations are called taxis relations (Definition \ref{def:taxis}). 
    Currently, the Parsimonious Vole parser implements a simple taxis analysis technique based on graph pattern matching, similar to the one described in Sections \ref{sec:graph-matching} and \ref{sec:pattern-based-operations}. %(employed for the MOOD and TRANSITIVITY feature enrichment. 
    Description of this work, however, is not included in this thesis because it has not yet been tested.%, so it is left for future work. 

    %As presented in Appendix \ref{ch:texis-patterns} I have created 
    A database of clause taxis patterns, represented as regular expressions, is listed in Appendix \ref{ch:texis-patterns}. It has been developed according to a systematisation in IFG 3 \citep{Halliday2004}. Each relation type has a set of patterns ascribed to it which represent clause order and presence or absence of explicit lexical markers or clause features. 

    In the taxis analysis process, each pair of adjacent clauses in the sentence is tested for compliance with TAXIS pattern in the database. The matches (there may be multiple ones for a single system feature) represent potential manifestations of the corresponding relation with no way to distinguish at the moment which pattern is, in fact, more likely to be correct. A similar problem was described for the TRANSITIVITY system and a potential solution was also described in terms of a discrimination mechanism in Section \ref{sec:systemic-evaluation-TRANSITIVITY}. More work, however, needs to be conducted in this area. 

\subsubsection{Dealing with covert elements and ellipsis}
    
    In the current approach to Transitivity parsing, accounting for the covert (or the so-called null) elements was taken as an instrumental goal to increase accuracy of parsing. Whether such elements should be accounted for in the grammar or whether they exist at all is still under debate in the linguistic literature, and, of course, arguments exist for and against the null elements.
    
    One future development would be to change the way graph patterns are generated from PTDB. The resulting graph patterns would need to be shaped such that the null elements are no longer a requirement for Transitivity parsing. This would, among other things, eliminate the need to create null element units in the constituency structure and would make the cross-theoretical links to GBT obsolete in this task. 

    I need to make, however, a reference here to \textit{ellipsis}, a well studied linguistic phenomenon. An elliptical construction is the omission from a clause of one or more words that are nevertheless understood in the context of the remaining elements. There is a variety of ellipsis types, among which are the null elements mentioned above. Whether to fill the gaps in the syntactic structure and which ones is a question that should not be abandoned too soon as providing rich and explicit structures can have positive outcomes in practical contexts. 

\subsubsection{Bridges to other grammars and linguistic theories}

    In this thesis exploration of cross-theoretical bridges is limited to two other traditions: that of Dependency grammars (specifically Stanford Dependency Grammar) and that of Phrase-Structure Grammars (specifically Government and Binding Theory). There is a wider set of useful cross-theoretical correspondences to establish that can materialise as positive reuse outcomes. 
    
    Due to compatible approaches to language analysis and because some work has already been done in this direction, among the most interesting correspondences would be Lexical Functional Grammars \citep{bresnan2015lexical}, Head-Driven Phrase Structure \citep{PollardSag1994}, Combinatory Categorial Grammar \citep{Steedman93,Steedman2000} and Tree Adjoining Grammars \citep{KrochJoshi85}, to name just a few. LFG has a functional layer in many respects similar to the functional layer of a (Nigel-style) SFG. Correspondences from TAG to SFG were already addressed by \citet{Yang-etal91} and in the last section of \citet{Bateman2008} in order to address among others the gap in the syntagmatic representation. \citet{BatemanTeich91} explored the possibility to adopt some aspects of HPSG for unit complexing. Having traced these new correspondences it will become possible to create the constituency backbone in the SFL style in a similar fashion as it is currently done from the Dependency Grammar. 

    The current implementation also requires an immediate upgrade to the latest version of the Stanford parser. Between 2006 and 2015 the Stanford parser \citep{Marneffe2006} was employing the Stanford dependency model for English (and a few other languages). Afterwards, in 2016, \citet{Nivre2016ud} proposed the language independent Universal Dependency scheme which was integrated into the Stanford Parser and replaced the Stanford dependency model. Around 2015--2016 the Parsimonious Vole parser was developed based on the Stanford dependency model. No transition to universal dependency was considered at that time because it was not mature or stable enough. For this reason the current thesis employs the legacy Stanford grammar and so a transition to universal dependency model must be considered in future work, in order to keep up the pace with the latest developments in the Stanford dependency parser. 


\subsubsection{Efficient graph rewriting method}
    
    In the current work the SFL style constituency backbone is created from dependency graphs. This is treated in computer science as \textit{graph/tree rewriting}. There is extensive literature addressing this task such as \citet{barendregt1987term}, \citet{courcelle1990graph}, \citet{plasmeijer1993functional} and \citet{grzegorz1999handbook}. 
    
    As at the time of developing the Parsimonious Vole parser I was exploring the precise properties necessary for transforming from the dependency into systemic functional constituency structures. Now that they are known, a more specific graph rewriting method can be considered. And so in the prototype implementation no pre-existing algorithm has been used. Future work needs to integrate the state of the art methods in graph rewriting and potentially improve or replace the current graph rewriting algorithm. Such a decision would need to be based on the efficiency and ease of providing the transformation rules.

\subsubsection{Execution order of graph patterns}  
    
    For a given constituency structure the current enrichment mechanism fires all the available graph patterns and any of the matching ones enrich the constituency structure. This can be costly when the number of patterns increases dramatically. Such a risk is imminent if, for example, the graph patterns are generated from the Nigel grammar as mentioned above. That richness poses the danger that too many graph patterns will make the parsing if not uncomputable, then at least too slow to be practical. 
    
    This risk can be countered, to an extent, by putting in place a selection mechanism that would seek to minimise the number of fired graph patterns for a given constituent unit. Such a mechanism needs to implement a search mechanism in the space of features covered by the graph patterns taking into account the systemic dependency between features and, therefore, between patterns. Moreover, a fitness function measuring information gain per graph and execution cost must be considered. Such a mechanism may already speed up the current implementation to an extent. 
    
\subsubsection{Dealing with multiple patterns per systemic feature}    

    In the current implementation for each process type configuration in PTDB multiple patterns graphs were generated. This is one of the leading causes of decreased TRANSITIVITY parsing accuracy as was described in Section \ref{sec:systemic-evaluation-TRANSITIVITY}.
    
    To prevent features from the same system from being assigned to constituent units simultaneously (even if clearly marked as a disjunctive set of possibilities) a discrimination mechanism should be implemented. Such a mechanism collects all the possible pattern matches first, and then assigns only the most suitable one to the constituent unit. This mechanism can be based on calculated probabilities or frequency in a corpus. More investigations are needed on these issues. 

\subsubsection{Analysis of errors from the current evaluation}     
    
    The evaluation performed in the current work does not go into detail analysing the types of errors the parser commits. In order to improve the performance of the current implementation the known errors need to be investigated down to the level of transformation rules, graph pattern and systemic feature disjunctions. Therefore, it is essential to carry on further investigation of segmentation errors (e.g. distance distribution for each feature) and errors in the constituency structure (false positives in the parser generated analysis and true negatives in the corpus). Results of a deeper error analysis will give information concerning how to correct the transformation rules from the dependency into SF constituency structures. Similar benefits can be achieved by investigating the errors in the systemic feature assignments.

\subsubsection{Investigation of probabilistic logics for SFG parsing}
    
    The problems of computational complexity in parsing with SFGs is explained in Chapter \ref{ch:introduction} and treated at length in \citet{Bateman2008}. At the heart of this problem lies the combinatorial explosion caused by the complex network of disjunctive systems. One way to deal with large combinatorial spaces is by using search approximations. For logical systems such an approximation is materialised in the form of probabilistic logics.
    
    Martin Kay was the first to attempt formalisation of systemic functional syntagmatic structures that would become known as Functional Unification Grammar (FUG) \citep{Kay1985}. This formalisation was adopted in other linguistic frameworks such as HPSG and Lexical Functional Grammars. For SFGs, however, using first order or even description logic reasoners has been shown to have severe complexity problems \citep{Bateman2008}. Employing probabilistic logics, therefore, may offer a further way of overcoming that complexity issue.
    
    Markov Logic \citep{Richardson2006,Domingos2010} draws my attention in particular, which I consider a good candidate for parsing with SFGs. It is a probabilistic logic, which applies ideas of Markov networks to first order logic enabling inference under uncertainty. What is very interesting about this logic is that tools implementing it have learning capabilities not only of formulas weights but also of new logical clauses. Moreover, it has been shown to be computationally feasible on large knowledge bases. The extent of such clauses should, however, still be investigated. 
    
    Markov logics can be employed, in the context of the current work, for addressing the graph pattern creation problem. Besides creating the graph patterns manually or from existing resources such as PTDB or advanced grammars such as Nigel, another possibility worth exploring is learning them from a corpus. 
    
    % In the current approach I am using graph pattern matching techniques to generate a rich set of features for the constituent units. However creating those patterns is a considerable effort. 
    
    Markov Logic tools, such as Alchemy\footnote{\url{http://alchemy.cs.washington.edu/}}, Tuffy\footnote{\url{http://i.stanford.edu/hazy/hazy/tuffy/}} and others, also have machine learning capabilities. Since graph patterns can be expressed via first order functions and individuals, and assuming that a (richly) annotated corpus in SFL style is available, these tools could be employed in an experiment to inductively learn pattern structures (and features) from the corpus. The results of such an exercise are useful to validate or challenge the patterns implemented in the parser, or even discover new patterns.

    This suggestion resembles the Vertical Strips (VS) of \citet{ODonoghue1991a}. The similarity is the probabilistic learning of patterns from a corpus. The difference is that VS patterns are syntactic segment chains from the root node down to tree leafs while with machine learning more complex patterns can be learned independently of their position in the syntactic tree. %Moreover, such patterns can be bound to the specific feature set. 
    
\section{Practical applications}
    % todo revise
    
    A wide variety of tasks in natural language processing, such as document classification, topic detection, sentiment analysis, word sense disambiguation, do not need parsing. These are tasks that can achieve high performance and accuracy with no linguistic features or with shallow syntactic information such as lemmas or parts of speech by using powerful statistical or machine learning techniques. What these tasks have in common is that they generally train on a large corpus and then operate again on large input text to finally yield a prediction for a single feature or set of features that they have been trained for. Consider for example the existing methods for sentiment analysis: they often provide a value between -1 and 1 estimating the sentiment polarity for a text that can be anything from one word to a whole page. 
 
    Conversely, there are tasks where extracting from texts (usually short) as much knowledge as possible is crucial for the success of the task. Consider a dialogue system, where deep understanding is essential for a meaningful, engaging and close to natural interaction with a human subject. It is no longer enough to assign a few shallow features to the input text, but a deep understanding is required for planning a proper response. Or consider the case of information extraction or relationship mining tasks, when knowledge is extracted at the sub-sentential level. In these scenarios the deeper linguistic understanding possible the better. 

    A parser of the type aimed at in this thesis would be useful to solve the latter set of tasks. The rich constituency parses could be an essential ingredient for further tasks such as anaphora resolution, clausal taxis analysis, rhetoric relation parsing, speech act detection, discourse model generation, knowledge extraction. All these tasks are needed for creating an intelligent interactive agent for various domains such as call centres, ticketing agencies, intelligent cars and houses, personal companions or assistants. 

    In marketing research, understanding the clients needs is one of the primary tasks. Mining intelligence from the unstructured data sources such as forums, customer reviews and social media posts is a particularly difficult task. In these cases the more features are available in the analysis the better. Employing parsers that offer deep feature rich outputs such as Parsimonious Vole satisfies this need. With the help of statistical methods feature correlations, predictive models and interpretations can be conveyed for the potential task at hand such as satisfaction level, requirement or complaint discovery.
    
\section{Final word}

    In this work I have advanced the work on automatic text analysis in SFL style. The current implementation did not succeed to employ a full SF grammar, and, just like previous attempts, had to accept limitations in the grammar size while maintaining broad language coverage. This task is particularly difficult because of the richness of such grammars. Nonetheless, modern applications desperately need deep feature-rich text analysis functionalities. 
    
    My view is that building on top of successful results achieved with other grammars by mapping them to parts of SF grammar constitutes a viable solution to the creation of SFL style constituency structures. Furthermore, employing graph patterns to enrich the structure with systemic features is the key ingredient for performing a delicate feature-rich text analysis.
    
    By further advancing the proposed methods and exploring new ways to cut through complexity, my hope is that one day automatically generating feature-rich text analysis will become the \textit{de facto} approach employed in truly intelligent agents that can, to a large extent, do with language what people do.